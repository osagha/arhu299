[
  {
    "objectID": "pages/lecturenotes/day2.html",
    "href": "pages/lecturenotes/day2.html",
    "title": "Day 2",
    "section": "",
    "text": "Today we built a one-layer sigmoid classifier to classify movie reviews into two classes: positive and negative."
  },
  {
    "objectID": "pages/lecturenotes/day2.html#content",
    "href": "pages/lecturenotes/day2.html#content",
    "title": "Day 2",
    "section": "",
    "text": "Today we built a one-layer sigmoid classifier to classify movie reviews into two classes: positive and negative."
  },
  {
    "objectID": "pages/lecturenotes/day2.html#model-setup",
    "href": "pages/lecturenotes/day2.html#model-setup",
    "title": "Day 2",
    "section": "Model setup",
    "text": "Model setup\nThe goal was to generate a score \\[z = xw + b\\] for each movie review, and tweak the formula behind the scores so that positive reviews have higher scores and negative reviews have lower scores.\nThe score \\(z\\) can be any real number. But what we actually want is for the model to guess whether the review is positive or negative. To do this, we use the sigmoid function, which squishes the real number line into the interval \\((0,1)\\).\n\\[ \\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\]\nThe model’s output is \\(\\hat y = \\sigma(z)\\), a number between 0 and 1, which we can interpret as “the probability that review \\(x\\) was a positive review”.\nNOTE: The symbol \\(\\sigma\\) is the greek letter sigma. The sigmoid function is also known as the logistic function."
  },
  {
    "objectID": "pages/lecturenotes/day2.html#hand-crafted-features",
    "href": "pages/lecturenotes/day2.html#hand-crafted-features",
    "title": "Day 2",
    "section": "Hand-crafted features",
    "text": "Hand-crafted features\nTo make text digestible by machine models, we have to encode it using feature representations. In class, we discussed the difference between hand-crafted features, which are created by the modeller, and learned features, which are learned from data.\nWe came up with some fake movie reviews and discussed three hand-crafted features: total word count, count of positive words, and count of negative words.\nEach review is encoded as a row of three numbers, representing the word count, the number of positive words, and the number of negative words, respectively. Here are four examples.\n\n\n\n\nflowchart LR\n    W[\"+, I was happy with this great movie\"] --&gt; A[\"[6, 2, 0]\"]\n    X[\"- , That was terrible\"] --&gt; B[\"[3, 0, 1]\"]\n    Y[\"+, Great movie\"] --&gt; C[\"[2, 1, 0]\"]\n    Z[\"-, The terrible acting actually made it fun to watch\"] --&gt; D[\"[7, 1, 1]\"]"
  },
  {
    "objectID": "pages/lecturenotes/day2.html#matrix-multiplication",
    "href": "pages/lecturenotes/day2.html#matrix-multiplication",
    "title": "Day 2",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nTo calculate the scores for multiple reviews at once, we use matrix multiplication. Each feature representation is a row of 3 numbers, so if we have 4 reviews in the dataset, then we end up with a \\(4\\times 3\\) matrix representing the dataset. \\(y\\) is a vector representing the true labels of each example, 1 for positive and 0 for negative.\n\\[y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x = \\left[\\begin{array}{ccc} 6 & 2 & 0 \\\\ 3 & 0 & 1 \\\\ 2 & 1 & 0 \\\\ 7 & 1 & 1 \\end{array}\\right]\\]\nOur weights will be a \\(3 \\times 1\\) matrix, and our bias will be a single number. I didn’t show this in class, but we have to write the bias number 4 times in a column vector, so that it has the same shape as \\(xw\\) (\\(4 \\times 1\\)).\nHere’s an example weight matrix (in this case the weights are a \\(3 \\times 1\\) column vector).\n\\[w = \\left[\\begin{array}{ccc} -1 \\\\ 2 \\\\ -2 \\end{array}\\right]\\]\n(In class we made the weight for the word count positive, but if we think that negative reviews have higher word counts, then we should actually make it negative.)\nIf our bias is \\(+4\\), then we end up with the following \\(z\\) scores.\n\\[z = xw + b = \\left[\\begin{array}{ccc} 6 & 2 & 0 \\\\ 3 & 0 & 1 \\\\ 2 & 1 & 0 \\\\ 7 & 1 & 1 \\end{array}\\right] \\left[\\begin{array}{ccc} -1 \\\\ 2 \\\\ -2 \\end{array}\\right] + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right]\\]\n\\[= \\left[\\begin{array}{ccc} 6 \\cdot (-1) + 2 \\cdot 2 + 0 \\cdot (-2) \\\\ 3 \\cdot (-1) + 0 \\cdot 2 + 1 \\cdot (-2) \\\\ 2 \\cdot (-1) + 1 \\cdot 2 + 0 \\cdot (-2) \\\\ 7 \\cdot (-1) + 1 \\cdot 2 + 1 \\cdot (-2) \\end{array}\\right]  + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right] = \\left[\\begin{array}{ccc} -2 \\\\ -5 \\\\ 0 \\\\ -7 \\end{array}\\right]  + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right] = \\left[\\begin{array}{ccc} 2 \\\\ -1 \\\\ 4 \\\\ -3 \\end{array}\\right]\\]\nWith these weights and biases, we end up with positive \\(z\\) for reviews 1 and 3, and negative \\(z\\) for reviews 2 and 4. The sigmoid \\(\\sigma\\) function will turn the positive \\(z\\) scores into probabilities greater than 0.5, and negative z scores into probabilities smaller than 0.5. Sigmoid takes in a single number, so we apply it element-wise (meaning, to each element of the vector).\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 4 \\\\ -3 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} \\sigma(2) \\\\ \\sigma(-1) \\\\ \\sigma(4) \\\\ \\sigma(-3) \\end{array}\\right] = \\left[\\begin{array}{c} 0.88 \\\\ 0.27 \\\\ 0.95 \\\\ 0.05 \\end{array}\\right]\\]\nRecall that the true labels were:\n\\[y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right]\\]\nWe can look at the differences between the true labels \\(y\\) and the predicted labels \\(\\hat y\\). The goal is to approximate the true labels as closely as possible, so the smaller these differences are, the better our model is.\n\\[y  - \\hat y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] - \\left[\\begin{array}{c} 0.88 \\\\ 0.27 \\\\ 0.95 \\\\ 0.05 \\end{array}\\right] \\,\\,\\,\\, = \\,\\,\\,\\, \\left[\\begin{array}{c} 0.12 \\\\ -0.27 \\\\ 0.05 \\\\ -0.05 \\end{array}\\right]\\]"
  },
  {
    "objectID": "pages/lecturenotes/day2.html#summing-up",
    "href": "pages/lecturenotes/day2.html#summing-up",
    "title": "Day 2",
    "section": "Summing up",
    "text": "Summing up\nWe now know how to write down the formulas for one-layer sigmoid classifier and apply it to binary text classification. In the future, we’ll look at the softmax function for multi-class classification (where there are more than two output labels).\nWe’ll also start to look at representation learning, the art of learning features from data (rather than hand-crafting them, like we’ve done here)."
  },
  {
    "objectID": "pages/lecturenotes/index.html",
    "href": "pages/lecturenotes/index.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "Lecture Notes\n\nDay 1: Intro, Text Encoding, Features, Classification\n\n\nDay 2: Binary classification with hand-crafted features\n\n\nDay 3: Learning bag-of-words features\n\n\nDay 4: TFIDF encoding, part 1\n\n\n2/8 - Day 5 - Notebook 0: Intro to Python\n\n\n2/13 - Day 6 - Notebook 1 More Python basics\n\n\n2/15 - Day 7 - Notebook 2 Intro to PyTorch"
  },
  {
    "objectID": "pages/lecturenotes/day1.html",
    "href": "pages/lecturenotes/day1.html",
    "title": "Day 1",
    "section": "",
    "text": "A neural network is a kind of function. It can take inputs in array format, and return outputs in a (possibly different) array format.\nNeural networks can take many kinds of inputs, like text, images, or sound files. In each case, the input has to be encoded as an array of numbers.\n\n\n\n\nflowchart LR\n    A(Text) --&gt; Z{Neural network}\n    B(Image) --&gt; Z\n    C(Sound file) --&gt; Z\n    Z --&gt; D(\"Label (Classification)\")\n    Z --&gt; E(\"Text, Image, Sound (Generation)\")\n    \n\n\n\n\n\nIn the case of classification, the output is a label. In the case of generation, the output could be of many types, including more text, images, or sound.\n\n\n\nOn a computer, all text is an encoded sequence of characters. A character is a symbol like a letter or number, punctuation, spaces/tabs/newlines, or special characters like “?”” or emojis.\nUnicode is the global standard for text encoding. It assigns every character to a code point. For example, capital A is U+0041, lowercase a is U+0061, and the number 1 is U+0031. Unicode covers all symbols in all scripts, plus many other characters. Some other examples are below.\n\n\n\nSymbol\nCode point\n\n\n\n\n⽔\nU+2F54\n\n\n(Space)\nU+0020\n\n\nअ\nU+0905\n\n\n?\nU+003F\n\n\n\nYou can search for any symbol you like here and find its Unicode code point.\nCode points are turned into machine-readable representations by particular encoding schemes, which differ in how much memory they use. The most popular is UTF-8 (read more about this on the Wiki link).\n\n\n\nLet’s take text classification as an example. In class we talked about sentiment analysis, spam detection, and topic modeling, but there are many other subtypes too.\n\n\n\n\nflowchart TD\n    A(Text Classification) --- B(Sentiment analysis)\n    A --- C(Spam detection)\n    A --- D(Topic modeling)\n    A --- E(...)\n\n\n\n\n\nIn text classification, the input is a string (a sequence of characters). Let’s take sentiment analysis as an example.\n\n\n\n\nflowchart LR\n    A(\"Text string &lt;br&gt; `This movie was great`\") --&gt; Z{Neural network}\n    Z --&gt; D(\"Positive ✓\")\n    Z --&gt; E(\"Negative ✗\")\n\n\n\n\n\nIn sentiment analysis, a model takes in a string and outputs a label, either positive or negative.\nIn practice, the model is rarely 100% sure about the label, so it outputs probabilities. For example, if 'This movie was great.' is the input, the model might output 0.9 for positive and 0.1 for negative.\nNext class, we’ll see a worked out example with a simple toy model.\n\n\nIf we’re doing text classification in English, then the vocabulary would consist of all English words (in the dictionary, let’s say).\nEach word in the vocabulary is just a sequence of characters, as far as the computer is concerned. It doesn’t know that dog is more similar in meaning to canine than it is to philosophy—all it sees are the characters.\nWe need a feature representation of words that captures similarities and differences in meaning. So, we encode every word as a vector, a list of numbers. These numbers can be thought of as coordinates for points in a high-dimensional space.\nNo matter how many dimensions a vector space has, we can always calculate the distance between points in the space.\n\n\n\nThe job of vector space embedding is to assign coordinates to every word in the vocabulary, and do this so that similar words are relatively close together and dissimilar words are relatively far apart.\nTo see an example, here are some 200 dimensional word embeddings that have been projected down into 3 dimensions. (Think about shadow puppets: 3D objects project as 2D shadows on a flat surface. This is kind of like looking at 3D shadows of 200D points.)\nYou can zoom and pan around and look at which words are closer together.\n\n\n\n\nNext class, we will see a toy example of text classification with low-dimensional feature representations of strings. From there, we will look at how higher-dimensional feature representations help models understand more about the meaning of the text."
  },
  {
    "objectID": "pages/lecturenotes/day1.html#what-is-a-neural-network-conceptually",
    "href": "pages/lecturenotes/day1.html#what-is-a-neural-network-conceptually",
    "title": "Day 1",
    "section": "",
    "text": "A neural network is a kind of function. It can take inputs in array format, and return outputs in a (possibly different) array format.\nNeural networks can take many kinds of inputs, like text, images, or sound files. In each case, the input has to be encoded as an array of numbers.\n\n\n\n\nflowchart LR\n    A(Text) --&gt; Z{Neural network}\n    B(Image) --&gt; Z\n    C(Sound file) --&gt; Z\n    Z --&gt; D(\"Label (Classification)\")\n    Z --&gt; E(\"Text, Image, Sound (Generation)\")\n    \n\n\n\n\n\nIn the case of classification, the output is a label. In the case of generation, the output could be of many types, including more text, images, or sound."
  },
  {
    "objectID": "pages/lecturenotes/day1.html#what-is-text",
    "href": "pages/lecturenotes/day1.html#what-is-text",
    "title": "Day 1",
    "section": "",
    "text": "On a computer, all text is an encoded sequence of characters. A character is a symbol like a letter or number, punctuation, spaces/tabs/newlines, or special characters like “?”” or emojis.\nUnicode is the global standard for text encoding. It assigns every character to a code point. For example, capital A is U+0041, lowercase a is U+0061, and the number 1 is U+0031. Unicode covers all symbols in all scripts, plus many other characters. Some other examples are below.\n\n\n\nSymbol\nCode point\n\n\n\n\n⽔\nU+2F54\n\n\n(Space)\nU+0020\n\n\nअ\nU+0905\n\n\n?\nU+003F\n\n\n\nYou can search for any symbol you like here and find its Unicode code point.\nCode points are turned into machine-readable representations by particular encoding schemes, which differ in how much memory they use. The most popular is UTF-8 (read more about this on the Wiki link)."
  },
  {
    "objectID": "pages/lecturenotes/day1.html#text-classification",
    "href": "pages/lecturenotes/day1.html#text-classification",
    "title": "Day 1",
    "section": "",
    "text": "Let’s take text classification as an example. In class we talked about sentiment analysis, spam detection, and topic modeling, but there are many other subtypes too.\n\n\n\n\nflowchart TD\n    A(Text Classification) --- B(Sentiment analysis)\n    A --- C(Spam detection)\n    A --- D(Topic modeling)\n    A --- E(...)\n\n\n\n\n\nIn text classification, the input is a string (a sequence of characters). Let’s take sentiment analysis as an example.\n\n\n\n\nflowchart LR\n    A(\"Text string &lt;br&gt; `This movie was great`\") --&gt; Z{Neural network}\n    Z --&gt; D(\"Positive ✓\")\n    Z --&gt; E(\"Negative ✗\")\n\n\n\n\n\nIn sentiment analysis, a model takes in a string and outputs a label, either positive or negative.\nIn practice, the model is rarely 100% sure about the label, so it outputs probabilities. For example, if 'This movie was great.' is the input, the model might output 0.9 for positive and 0.1 for negative.\nNext class, we’ll see a worked out example with a simple toy model.\n\n\nIf we’re doing text classification in English, then the vocabulary would consist of all English words (in the dictionary, let’s say).\nEach word in the vocabulary is just a sequence of characters, as far as the computer is concerned. It doesn’t know that dog is more similar in meaning to canine than it is to philosophy—all it sees are the characters.\nWe need a feature representation of words that captures similarities and differences in meaning. So, we encode every word as a vector, a list of numbers. These numbers can be thought of as coordinates for points in a high-dimensional space.\nNo matter how many dimensions a vector space has, we can always calculate the distance between points in the space.\n\n\n\nThe job of vector space embedding is to assign coordinates to every word in the vocabulary, and do this so that similar words are relatively close together and dissimilar words are relatively far apart.\nTo see an example, here are some 200 dimensional word embeddings that have been projected down into 3 dimensions. (Think about shadow puppets: 3D objects project as 2D shadows on a flat surface. This is kind of like looking at 3D shadows of 200D points.)\nYou can zoom and pan around and look at which words are closer together."
  },
  {
    "objectID": "pages/lecturenotes/day1.html#for-next-time",
    "href": "pages/lecturenotes/day1.html#for-next-time",
    "title": "Day 1",
    "section": "",
    "text": "Next class, we will see a toy example of text classification with low-dimensional feature representations of strings. From there, we will look at how higher-dimensional feature representations help models understand more about the meaning of the text."
  },
  {
    "objectID": "pages/homework/homework1.html",
    "href": "pages/homework/homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Due Thursday February 8th.\nSolve each problem by hand. You may handwrite or type your solutions.\nIf you handwrite, your writing must be clearly legible.\n\n\nHere are some matrices of various shapes and sizes. \\[I_2 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, I_3 = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, P = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, Q = \\left[\\begin{array}{ccc} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{array}\\right]\\]\n\\[ V = \\left[\\begin{array}{ccc} 7 & 0 & -3  \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, W = \\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\\right] \\]\n\\[ A = \\left[\\begin{array}{ccc} -2 & 3 & 8 \\\\ 0 & 11 & -3 \\\\ -2 & 0 & 0 \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, B = \\left[\\begin{array}{ccc} 1 & 1 & -4 \\\\ -1 & 0 & -2  \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, C = \\left[\\begin{array}{cc} 0 & 3  \\\\ -1 & -5 \\\\ 5 & 7  \\end{array}\\right] \\]\n\nCalculate the matrix products \\(I_2B\\), \\(I_3A\\), \\(PA\\), and \\(QA\\).\nIn plain words, what do the matrices \\(I_2\\), \\(I_3\\), \\(P\\), and \\(Q\\) do to other matrices?\nCalculate the matrix products \\(VW\\) and \\(WV\\).\nWhat do you notice about \\(VW\\) and \\(WV\\)?\nThink about the products \\(AB\\), \\(BA\\), \\(AC\\), \\(CA\\), \\(BC\\),and \\(CB\\). Which of these products work?\nFor each product that works, calculate it.\n\n\n\n\nRead the lecture notes for Day 2 carefully, then do all of the following by hand.\n\nWrite down the weights and biases for that model.\nMake up five short movie reviews and count up their features (word count, positive word count, negative word count). Make sure you write down which words you consider positive and which words you consider negative!\nWrite down the \\(5 \\times 3\\) matrix \\(x\\) for your encoded data and a \\(5 \\times 1\\) column vector \\(y\\) for the true labels.\nUsing your own data matrix \\(x\\), along with the weights and biases from the Day 2 lecture notes, calculate the \\(z = xw + b\\).\nThen, calculate the model predictions \\(\\hat y = \\sigma(z)\\). (You can use a calculator for the sigmoid part.)\nCalculate the difference \\(y - \\hat y\\).\nHow did the model do on your data?\nCan you change the bias or the weights to improve the performance?"
  },
  {
    "objectID": "pages/homework/homework1.html#part-1-matrix-math-warmup",
    "href": "pages/homework/homework1.html#part-1-matrix-math-warmup",
    "title": "Homework 1",
    "section": "",
    "text": "Here are some matrices of various shapes and sizes. \\[I_2 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, I_3 = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, P = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, Q = \\left[\\begin{array}{ccc} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{array}\\right]\\]\n\\[ V = \\left[\\begin{array}{ccc} 7 & 0 & -3  \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, W = \\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\\right] \\]\n\\[ A = \\left[\\begin{array}{ccc} -2 & 3 & 8 \\\\ 0 & 11 & -3 \\\\ -2 & 0 & 0 \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, B = \\left[\\begin{array}{ccc} 1 & 1 & -4 \\\\ -1 & 0 & -2  \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, C = \\left[\\begin{array}{cc} 0 & 3  \\\\ -1 & -5 \\\\ 5 & 7  \\end{array}\\right] \\]\n\nCalculate the matrix products \\(I_2B\\), \\(I_3A\\), \\(PA\\), and \\(QA\\).\nIn plain words, what do the matrices \\(I_2\\), \\(I_3\\), \\(P\\), and \\(Q\\) do to other matrices?\nCalculate the matrix products \\(VW\\) and \\(WV\\).\nWhat do you notice about \\(VW\\) and \\(WV\\)?\nThink about the products \\(AB\\), \\(BA\\), \\(AC\\), \\(CA\\), \\(BC\\),and \\(CB\\). Which of these products work?\nFor each product that works, calculate it."
  },
  {
    "objectID": "pages/homework/homework1.html#part-2-more-binary-classification",
    "href": "pages/homework/homework1.html#part-2-more-binary-classification",
    "title": "Homework 1",
    "section": "",
    "text": "Read the lecture notes for Day 2 carefully, then do all of the following by hand.\n\nWrite down the weights and biases for that model.\nMake up five short movie reviews and count up their features (word count, positive word count, negative word count). Make sure you write down which words you consider positive and which words you consider negative!\nWrite down the \\(5 \\times 3\\) matrix \\(x\\) for your encoded data and a \\(5 \\times 1\\) column vector \\(y\\) for the true labels.\nUsing your own data matrix \\(x\\), along with the weights and biases from the Day 2 lecture notes, calculate the \\(z = xw + b\\).\nThen, calculate the model predictions \\(\\hat y = \\sigma(z)\\). (You can use a calculator for the sigmoid part.)\nCalculate the difference \\(y - \\hat y\\).\nHow did the model do on your data?\nCan you change the bias or the weights to improve the performance?"
  },
  {
    "objectID": "pages/homework/homework3.html",
    "href": "pages/homework/homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Write your own code! Do not just copy code from the lecture notes or from the internet.\nYou may look at online resources for help. But write everything yourself.\nIf you are serious about learning this stuff, you need to build muscle memory for writing code, just like you need muscle memory for playing the piano.\n\n\nWrite a function that takes a bag-of-words matrix and returns a row vector (\\(1\\times V\\)) containing the document frequencies for each word.\nWrite a function that takes a bag-of-words matrix and returns the tf-idf matrix. (Hint: The function needs to get the DF and IDF, and then multiply the IDF through the bag-of-words matrix.)\nWrite a function that takes an \\(N\\times V\\) matrix \\(x\\) and a \\(V\\times 1\\) weight vector \\(w\\), and a bias \\(b\\), and returns the model predictions. (Hint: You need to calculate \\(z = xw + b\\), and then return the model predictions \\(\\hat y = \\sigma(z)\\).)\n\n\n\nWrite a function that takes \\(y\\) (an \\(N \\times 1\\) column vector of true labels) and \\(\\hat y\\) (an \\(N\\times 1\\) column vector of predictions) and calculates the cross-entropy loss. Test it using randomly generated arrays.\nWrite a function that takes a number \\(V\\) (the vocab size) and generates 5 random \\(V \\times 1\\) column vectors of weights. (Use a loop.) Have it return the vectors in a tuple."
  },
  {
    "objectID": "pages/homework/homework3.html#tf-idf-in-python",
    "href": "pages/homework/homework3.html#tf-idf-in-python",
    "title": "Homework 3",
    "section": "",
    "text": "Write your own code! Do not just copy code from the lecture notes or from the internet.\nYou may look at online resources for help. But write everything yourself.\nIf you are serious about learning this stuff, you need to build muscle memory for writing code, just like you need muscle memory for playing the piano.\n\n\nWrite a function that takes a bag-of-words matrix and returns a row vector (\\(1\\times V\\)) containing the document frequencies for each word.\nWrite a function that takes a bag-of-words matrix and returns the tf-idf matrix. (Hint: The function needs to get the DF and IDF, and then multiply the IDF through the bag-of-words matrix.)\nWrite a function that takes an \\(N\\times V\\) matrix \\(x\\) and a \\(V\\times 1\\) weight vector \\(w\\), and a bias \\(b\\), and returns the model predictions. (Hint: You need to calculate \\(z = xw + b\\), and then return the model predictions \\(\\hat y = \\sigma(z)\\).)\n\n\n\nWrite a function that takes \\(y\\) (an \\(N \\times 1\\) column vector of true labels) and \\(\\hat y\\) (an \\(N\\times 1\\) column vector of predictions) and calculates the cross-entropy loss. Test it using randomly generated arrays.\nWrite a function that takes a number \\(V\\) (the vocab size) and generates 5 random \\(V \\times 1\\) column vectors of weights. (Use a loop.) Have it return the vectors in a tuple."
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Resources",
    "section": "",
    "text": "I’ll be adding resources here throughout the semester.\n\n\n\n\nA tried-and-true standard textbook for computational linguistics and NLP. The book’s website includes slides too, which are great for a quick reference. Accompanying Youtube lectures can be found here.\n\n\n\nA high quality, detailed, and somewhat technical introduction to NLP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brilliant visual introduction to deep learning by 3blue1brown (Grant Sanderson). See also his interactive courses on neural networks, linear algebra, and calculus.\n\n\n\n\n\n\nA Stanford NLP course by Dan Jurafsky, based on the textbook Speech and Language Processing linked above. More accessible than the other courses listed below.\n\n\n\nA Stanford course by Christopher Manning, free on Youtube. This is aimed at Computer Science majors, so it may be more accessible after you have already learned the basics in this course.\n\n\n\nA UC Berkeley course on computer vision, free on Youtube. Quite technical, and best approached after this class."
  },
  {
    "objectID": "pages/resources.html#textbooks",
    "href": "pages/resources.html#textbooks",
    "title": "Resources",
    "section": "",
    "text": "A tried-and-true standard textbook for computational linguistics and NLP. The book’s website includes slides too, which are great for a quick reference. Accompanying Youtube lectures can be found here.\n\n\n\nA high quality, detailed, and somewhat technical introduction to NLP."
  },
  {
    "objectID": "pages/resources.html#videos",
    "href": "pages/resources.html#videos",
    "title": "Resources",
    "section": "",
    "text": "A brilliant visual introduction to deep learning by 3blue1brown (Grant Sanderson). See also his interactive courses on neural networks, linear algebra, and calculus."
  },
  {
    "objectID": "pages/resources.html#full-courses",
    "href": "pages/resources.html#full-courses",
    "title": "Resources",
    "section": "",
    "text": "A Stanford NLP course by Dan Jurafsky, based on the textbook Speech and Language Processing linked above. More accessible than the other courses listed below.\n\n\n\nA Stanford course by Christopher Manning, free on Youtube. This is aimed at Computer Science majors, so it may be more accessible after you have already learned the basics in this course.\n\n\n\nA UC Berkeley course on computer vision, free on Youtube. Quite technical, and best approached after this class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ARHU299",
    "section": "",
    "text": "This is the homepage for ARHU299: Machine Learning in Language and Art at the University of Maryland, College Park.\nPlease see the syllabus for more info."
  },
  {
    "objectID": "pages/syllabus.html",
    "href": "pages/syllabus.html",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Syllabus for ARHU299: Machine Learning in Language and Art at the University of Maryland, College Park.\nLast updated January 24, 2024.\nInstructor: Omar Agha (oagha@umd.edu)\nTA: Utku Türk (utkuturk@umd.edu)\nOffice hours:\nOmar Agha: Monday 3-4pm in MMH 3416A (outer door is locked; email Omar to be let in)\nUtku Türk: Wednesday 4-5pm in MMH 1407C (outer door is locked; email Utku to be let in)\n\n\nIn this course, you will learn how machine learning models read and write text, classify images, and more. This requires understanding some basic elements of linear algebra (vectors and matrices), calculus (the chain rule for derivatives), Python programming, and basic statistics. If you have not seen any of this before, you will learn the relevant tools along the way, in the context of solving applied problems, and with the aid of software that performs complex calculations for you.\nThis is a fast-paced class. There will be homework every week, and frequent in-class assignments. You will need to make a serious effort to learn about difficult technical topics. I assume you are here because you want to learn this material, and I will do everything I can to help you succeed.\nWe will work up from simple models to more complex ones. We will start with the simplest neural network architecture, the perceptron, and apply it to text classification tasks. After that, we will study deeper networks (multi layer perceptrons) that can be used for language modeling (predicting the next word).\nBy the end of the course, we will cover the basics of transformer models and convolutional neural networks. Transformers are the building blocks of language models like GPT, and convolutional NNs are used for classifying and generating images. Depending on the speed of your progress, we might also have time to discuss more advanced neural network architectures like latent diffusion models, which are behind powerful text-to-image generators like Stable Diffusion and Midjourney.\n\n\n\nEngineers and scientists who work on these topics usually do years of intense study in math and computer science before reaching proficiency. So it’s not realistic for us to go that deep in one semester.\nLuckily, building simple text and image models (and maybe music generators) does not require as much expertise, only some Python skills and enough curiosity to figure things out on one’s own.\n\n\n\nYou will learn how to multiply matrices, use probability, and understand simple equations and graphs involving functions like \\(\\sin\\), \\(\\cos\\), and \\(\\log\\). You will see how these concepts are actually applied in real-world problems.\nMachine learning models use calculus (derivatives) under the hood, and you will learn the basics of how this works. But for the most part. we will rely on software to do those calculations for us.\n\n\n\nYou will learn how to use the Python programming language, along with some Python libraries. The most important libraries are PyTorch and Numpy, for matrix math, and Pandas, for data processing. We will also use Matplotlib, Seaborn, and Plotly for plotting, and Pillow for image processing.\nThe basic Python concepts you will learn are functions, classes, lists, tuples, dictionaries, for-loops, list comprehensions, and lambda expressions.\nIf you want to get a head start on your Python journey, google all the unfamiliar terms in this section and read up.\n\n\n\nThere is no textbook, since I haven’t found one that fits the audience of this course. I will post optional resources (books, articles, and videos) in the lecture notes, and under the Resources section of the course website.\n\n\n\nI will post lecture notes within 48 hours after the relevant lecture. Lecture notes will usually not be available before class, so you must pay attention and take your own notes during class sessions.\nAll lecture notes will be posted under Lecture Notes. I will also post code in the form of Python Notebooks on the website. You can open a copy in Google Colab to study and play with the code.\n\n\n\n\n\nI will assume that you are here because you want to learn. This applies to all policies and requirements: I assume that you will come to class and complete the homework because you want to learn.\nI will do my best to not waste your time, and to not impose any requirements on you that are unnecessary for your educational goals.\n\n\n\nAttendance is not directly graded. However, there will be frequent in-class assignments, and participation is a part of your grade. Also, missing class will make it hard to keep up. For this reason, poor attendance will generally lower your grade and prevent you from learning effectively.\n\n\n\nIn this class you will learn math, programming, and critical thinking skills by practicing them. Learning this material without practice is not possible.\nI find that accepting late homework usually causes more students to fall behind, ultimately harming their grades and progress, even if it feels generous in the short term. So, late homework will not be accepted unless I have already granted you an extension.\nWe will mostly be coding in Google Colab. If you want to use another editor you can, but your submitted code must be in .ipynb format and it must run correctly in Colab. (So, make sure to upload your file in Colab and test it there.)\n\n\n\nIf you need an extension on any assignment, contact me before noon on the due date, and verify that I approve. (Better to reach out earlier than that if possible.)\nIf you need to be excused from class, contact me before that class starts.\nIf you are excused from class, I will let you know if there are any in-class assignments that you need to make up.\n\n\n\nThe UMD Code of Academic Integrity defines plagiarism as “representing the words or ideas of another as one’s own in any academic course, exercise or research”. If you copy text or code from any source, you must treat is as a quotation and provide the source. (You can either link to the source or provide a full bibliographic citation.)\nPlagiarism on any assignment will result in a zero. Repeat plagiarists will be reported to the Office of Student Conduct.\nCopying work from another student is plagiarism, but group work is encouraged. All submitted work must be your own.\n\n\n\nGenerative “AI” tools like ChatGPT or Bing produce text in a random process by sampling from the most likely next words.\nThey can be useful in certain contexts. However, the randomly generated text is often biased and false in subtle ways, and plagiarized from other sources. (This is because the model’s next-word-probabilities are derived from documents scraped from the internet.)\nCopying and pasting the output of generative AI on any assignment will be considered plagiarism. (See the above definition.)\nIf you choose to use generative AI to help with solving problems, that’s ok, but you must ensure that all submitted work is your own, and you are responsible for any errors. (Using ChatGPT or other software to edit your prose is also ok, especially if you are not a native speaker of English.)\n\n\n\n\n\n\n\n50% homework\n20% quizzes\n10% participation (in person and on Discord)\n20% project proposal\n\n\n\n\n\nBy the last day of class, please submit a 2-3 page project proposal. This can be for a product that you would like to create, or for a research project. In the proposal, please discuss in detail\n\nhow you would use the technologies we cover in class, (5%)\nwhat you think the most challenging components would be, (5%)\nwhat the likely social impact of the project would be. (5%)\n\nYou don’t need to turn in any working code. (You are welcome to do so, and you won’t be graded on the code, only the proposal.)\n\n\nBefore Mar 14, please schedule a meeting with me to talk through project ideas.\n\nMar 14: Turn in one paragraph on your project proposal. If you have multiple ideas, do a short paragraph for each.\nApr 11: Turn in a half page, more fleshed out version. Start to include some details about technologies, challenges, and/or social impact. (Worth 5% out of the full 20%.)\nMay 7: Final due date for project proposal.\n\n\n\n\n\n\n\nYour success is very important to me. I (or the TA) will answer your emails promptly (within 24 hours) and make time to meet with you if you need extra help.\nIf you need help, please contact me ASAP. Often, students fail because they wait too long to ask for help, and then fall too far behind to catch up. Don’t let this happen to you!\n\n\n\nIf you find yourself struggling to understand something, try re-reading the lecture notes or re-watching the assigned videos. Ideas often take multiple tries to click.\nI will link to helpful videos and articles in my lecture notes. Don’t forget to use them!\n\n\n\nGroup work is highly encouraged. Please try to find a study group early on, and try to meet up for each homework. I’ll make a Discord chat for the class to help you all collaborate. As a reminder, copying code or prose directly from other students is plagiarism, so please make sure that all submitted work is your own.\n\n\n\nTaking care of yourself is the foundation for all success in life. Make sure you come to class well-rested, nourished, and ready to learn. (If you need to eat or drink beverages in class, that’s fine with me.)\n\n\n\nIf mental health challenges are keeping you from doing your best, the university provides Counseling and Behavioral Health Services. The earlier you reach out, the more you can benefit from these resources.\n\n\n\n\nStudents with disabilities are very welcome in this class. If you require accommodations, please contact me as soon as possible. For more information about disability accommodations at UMD, please see the ADS webpage."
  },
  {
    "objectID": "pages/syllabus.html#course-summary",
    "href": "pages/syllabus.html#course-summary",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "In this course, you will learn how machine learning models read and write text, classify images, and more. This requires understanding some basic elements of linear algebra (vectors and matrices), calculus (the chain rule for derivatives), Python programming, and basic statistics. If you have not seen any of this before, you will learn the relevant tools along the way, in the context of solving applied problems, and with the aid of software that performs complex calculations for you.\nThis is a fast-paced class. There will be homework every week, and frequent in-class assignments. You will need to make a serious effort to learn about difficult technical topics. I assume you are here because you want to learn this material, and I will do everything I can to help you succeed.\nWe will work up from simple models to more complex ones. We will start with the simplest neural network architecture, the perceptron, and apply it to text classification tasks. After that, we will study deeper networks (multi layer perceptrons) that can be used for language modeling (predicting the next word).\nBy the end of the course, we will cover the basics of transformer models and convolutional neural networks. Transformers are the building blocks of language models like GPT, and convolutional NNs are used for classifying and generating images. Depending on the speed of your progress, we might also have time to discuss more advanced neural network architectures like latent diffusion models, which are behind powerful text-to-image generators like Stable Diffusion and Midjourney."
  },
  {
    "objectID": "pages/syllabus.html#how-deep-will-we-go",
    "href": "pages/syllabus.html#how-deep-will-we-go",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Engineers and scientists who work on these topics usually do years of intense study in math and computer science before reaching proficiency. So it’s not realistic for us to go that deep in one semester.\nLuckily, building simple text and image models (and maybe music generators) does not require as much expertise, only some Python skills and enough curiosity to figure things out on one’s own."
  },
  {
    "objectID": "pages/syllabus.html#math-skills",
    "href": "pages/syllabus.html#math-skills",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "You will learn how to multiply matrices, use probability, and understand simple equations and graphs involving functions like \\(\\sin\\), \\(\\cos\\), and \\(\\log\\). You will see how these concepts are actually applied in real-world problems.\nMachine learning models use calculus (derivatives) under the hood, and you will learn the basics of how this works. But for the most part. we will rely on software to do those calculations for us."
  },
  {
    "objectID": "pages/syllabus.html#programming-skills",
    "href": "pages/syllabus.html#programming-skills",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "You will learn how to use the Python programming language, along with some Python libraries. The most important libraries are PyTorch and Numpy, for matrix math, and Pandas, for data processing. We will also use Matplotlib, Seaborn, and Plotly for plotting, and Pillow for image processing.\nThe basic Python concepts you will learn are functions, classes, lists, tuples, dictionaries, for-loops, list comprehensions, and lambda expressions.\nIf you want to get a head start on your Python journey, google all the unfamiliar terms in this section and read up."
  },
  {
    "objectID": "pages/syllabus.html#resources",
    "href": "pages/syllabus.html#resources",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "There is no textbook, since I haven’t found one that fits the audience of this course. I will post optional resources (books, articles, and videos) in the lecture notes, and under the Resources section of the course website."
  },
  {
    "objectID": "pages/syllabus.html#lecture-notes",
    "href": "pages/syllabus.html#lecture-notes",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "I will post lecture notes within 48 hours after the relevant lecture. Lecture notes will usually not be available before class, so you must pay attention and take your own notes during class sessions.\nAll lecture notes will be posted under Lecture Notes. I will also post code in the form of Python Notebooks on the website. You can open a copy in Google Colab to study and play with the code."
  },
  {
    "objectID": "pages/syllabus.html#class-policies",
    "href": "pages/syllabus.html#class-policies",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "I will assume that you are here because you want to learn. This applies to all policies and requirements: I assume that you will come to class and complete the homework because you want to learn.\nI will do my best to not waste your time, and to not impose any requirements on you that are unnecessary for your educational goals.\n\n\n\nAttendance is not directly graded. However, there will be frequent in-class assignments, and participation is a part of your grade. Also, missing class will make it hard to keep up. For this reason, poor attendance will generally lower your grade and prevent you from learning effectively.\n\n\n\nIn this class you will learn math, programming, and critical thinking skills by practicing them. Learning this material without practice is not possible.\nI find that accepting late homework usually causes more students to fall behind, ultimately harming their grades and progress, even if it feels generous in the short term. So, late homework will not be accepted unless I have already granted you an extension.\nWe will mostly be coding in Google Colab. If you want to use another editor you can, but your submitted code must be in .ipynb format and it must run correctly in Colab. (So, make sure to upload your file in Colab and test it there.)\n\n\n\nIf you need an extension on any assignment, contact me before noon on the due date, and verify that I approve. (Better to reach out earlier than that if possible.)\nIf you need to be excused from class, contact me before that class starts.\nIf you are excused from class, I will let you know if there are any in-class assignments that you need to make up.\n\n\n\nThe UMD Code of Academic Integrity defines plagiarism as “representing the words or ideas of another as one’s own in any academic course, exercise or research”. If you copy text or code from any source, you must treat is as a quotation and provide the source. (You can either link to the source or provide a full bibliographic citation.)\nPlagiarism on any assignment will result in a zero. Repeat plagiarists will be reported to the Office of Student Conduct.\nCopying work from another student is plagiarism, but group work is encouraged. All submitted work must be your own.\n\n\n\nGenerative “AI” tools like ChatGPT or Bing produce text in a random process by sampling from the most likely next words.\nThey can be useful in certain contexts. However, the randomly generated text is often biased and false in subtle ways, and plagiarized from other sources. (This is because the model’s next-word-probabilities are derived from documents scraped from the internet.)\nCopying and pasting the output of generative AI on any assignment will be considered plagiarism. (See the above definition.)\nIf you choose to use generative AI to help with solving problems, that’s ok, but you must ensure that all submitted work is your own, and you are responsible for any errors. (Using ChatGPT or other software to edit your prose is also ok, especially if you are not a native speaker of English.)"
  },
  {
    "objectID": "pages/syllabus.html#grades",
    "href": "pages/syllabus.html#grades",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "50% homework\n20% quizzes\n10% participation (in person and on Discord)\n20% project proposal"
  },
  {
    "objectID": "pages/syllabus.html#project-proposal",
    "href": "pages/syllabus.html#project-proposal",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "By the last day of class, please submit a 2-3 page project proposal. This can be for a product that you would like to create, or for a research project. In the proposal, please discuss in detail\n\nhow you would use the technologies we cover in class, (5%)\nwhat you think the most challenging components would be, (5%)\nwhat the likely social impact of the project would be. (5%)\n\nYou don’t need to turn in any working code. (You are welcome to do so, and you won’t be graded on the code, only the proposal.)\n\n\nBefore Mar 14, please schedule a meeting with me to talk through project ideas.\n\nMar 14: Turn in one paragraph on your project proposal. If you have multiple ideas, do a short paragraph for each.\nApr 11: Turn in a half page, more fleshed out version. Start to include some details about technologies, challenges, and/or social impact. (Worth 5% out of the full 20%.)\nMay 7: Final due date for project proposal."
  },
  {
    "objectID": "pages/syllabus.html#how-to-succeed-in-this-class",
    "href": "pages/syllabus.html#how-to-succeed-in-this-class",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Your success is very important to me. I (or the TA) will answer your emails promptly (within 24 hours) and make time to meet with you if you need extra help.\nIf you need help, please contact me ASAP. Often, students fail because they wait too long to ask for help, and then fall too far behind to catch up. Don’t let this happen to you!\n\n\n\nIf you find yourself struggling to understand something, try re-reading the lecture notes or re-watching the assigned videos. Ideas often take multiple tries to click.\nI will link to helpful videos and articles in my lecture notes. Don’t forget to use them!\n\n\n\nGroup work is highly encouraged. Please try to find a study group early on, and try to meet up for each homework. I’ll make a Discord chat for the class to help you all collaborate. As a reminder, copying code or prose directly from other students is plagiarism, so please make sure that all submitted work is your own.\n\n\n\nTaking care of yourself is the foundation for all success in life. Make sure you come to class well-rested, nourished, and ready to learn. (If you need to eat or drink beverages in class, that’s fine with me.)\n\n\n\nIf mental health challenges are keeping you from doing your best, the university provides Counseling and Behavioral Health Services. The earlier you reach out, the more you can benefit from these resources."
  },
  {
    "objectID": "pages/syllabus.html#disability-accommodations",
    "href": "pages/syllabus.html#disability-accommodations",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Students with disabilities are very welcome in this class. If you require accommodations, please contact me as soon as possible. For more information about disability accommodations at UMD, please see the ADS webpage."
  },
  {
    "objectID": "pages/homework/homework2.html",
    "href": "pages/homework/homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "This homework has a by-hand component and a Python component. For the by-hand component, you can turn in a scan or a typed solution. For the Python component, turn in a .ipynb file."
  },
  {
    "objectID": "pages/homework/homework2.html#tf-idf-by-hand",
    "href": "pages/homework/homework2.html#tf-idf-by-hand",
    "title": "Homework 2",
    "section": "Tf-idf by hand",
    "text": "Tf-idf by hand\nMake up 5 short documents and construct the tf-idf matrix by hand for these 5 documents. Steps: 1. First make up your data. 2. Make the bag-of-words (document-term) matrix by counting the words in each document. 3. Then calculate the document frequency and inverse document frequency of each word. 4. Then divide each column by its IDF.\nUse the version without log-scaling:\n\\[TF[d,t] = count(d,t) \\,\\,\\,\\,\\,\\,\\,\\,\\, IDF[t] = \\frac{N}{DF[t]}\\] Show your work at each step!\nWhich words are the most discriminative words? (Meaning, which words appear in the fewest documents, and therefore provide the most information?)"
  },
  {
    "objectID": "pages/homework/homework2.html#python-warmup",
    "href": "pages/homework/homework2.html#python-warmup",
    "title": "Homework 2",
    "section": "Python warmup",
    "text": "Python warmup\n\nPart 1\nDefine the sigmoid function in Python. Use np.exp from the numpy library.\n\n# np is the standard abbreviation for numpy\nimport numpy as np\n\ndef sigmoid(z: float) -&gt; float:\n    # your code here...\n    return # ...\n\nWrite a function that takes a single string, splits it up on the spaces, and returns a set containing all the unique words. (Hint: Use the .split() method. You can turn a list x into a set using set(x).)\n\ndef split_into_set(string: str) -&gt; set[str]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = 'great movie'\ntest_output = set(['great', 'movie'])\nif split_into_set(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a list of strings and returns a set containing all the unique words for all the strings together. (This is the vocabulary construction step.) (Hint: You can use the previous function to help you.)\n\ndef get_word_set(list_of_strs: list[str]) -&gt; set[str]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = ['great movie', 'terrible movie']\ntest_output = set(['great', 'terrible', 'movie'])\nif get_word_set(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a single string and returns a dictionary whose keys are the unique words in the string, and whose values are the word counts. (Hint: use an if-else pattern inside a for-loop.)\n\ndef get_wc(string: str) -&gt; dict[str, int]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = 'great movie awesome movie'\ntest_output = {'great': 1, 'awesome': 1, 'movie': 2}\nif get_wc(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a list of strings and returns a dictionary whose keys are the unique words, and whose values are the word counts.\n\ndef get_wcs_from_list(list_of_strs: list[str]) -&gt; dict[str, int]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = ['great movie', 'terrible movie']\ntest_output = {'great': 1, 'terrible': 1, 'movie': 2}\nif get_wcs_from_list(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\n\n\nPart 2\nWrite a function that takes a list of shapes and returns a list of randomly generated PyTorch tensors, where each tensor’s shape comes from the input list. (Hint: Each shape is a tuple of ints, so the input is a list of tuples of ints. Use torch.rand(...).)\n\nimport torch\n\ndef gen_tensors(shapes : list[tuple[int, int]]) -&gt; list[torch.Tensor]:\n    # your code here...\n    return # ...\n\n# check your output\ntest_input = [(1,1), (2,3), (5,4)]\nprint(gen_tensors(test_input))\n\nNone\n\n\nWrite a function that takes a list of strings and returns the bag-of-words matrix as a PyTorch tensor. (Hint: The numbers in the tensor are word counts. This is a multi-step process. Use the functions you wrote in Part 1 to help you.)\n\nimport torch\n\ndef get_bow_matrix(list_of_strs: list[str]) -&gt; torch.Tensor:\n    # your code here ...\n    return # ...\n\n# check your output here\ntest_input = [\n    'great movie',\n    'what crap',\n    'what a great movie',\n]\nprint(get_bow_matrix(test_input))\n\nNone"
  },
  {
    "objectID": "pages/homework/homework2.html#next-time",
    "href": "pages/homework/homework2.html#next-time",
    "title": "Homework 2",
    "section": "Next time",
    "text": "Next time\nNext homework, we’ll put the pieces together to do TFIDF in Python!"
  },
  {
    "objectID": "pages/homework/index.html",
    "href": "pages/homework/index.html",
    "title": "Homework",
    "section": "",
    "text": "Homework\n\nHomework 1: Binary classification and matrix math\n\n\nHomework 2: TFIDF by hand, plus some Python"
  },
  {
    "objectID": "pages/lecturenotes/day4.html",
    "href": "pages/lecturenotes/day4.html",
    "title": "Day 4",
    "section": "",
    "text": "On 2/6/24, we reviewed the bag-of-words encoding for documents, and learned about a more interesting and useful encoding. The more useful one is called term frequency-inverse document frequency (tf-idf).\nIt is kind of like a bag-of-words encoding, but gives more weight to informative (discriminative) words, and less weight to non-informative (non-discriminative) words like the, and, etc, which are usually less helpful for classification.\n\n\n\nWhen we were looking at the bag-of-words encoding, we learned how to encode a dataset as a matrix like this one. This is called a document-term matrix, because each row represents a document, and each column represents a term (a word).\nHere is a subset of a document-term matrix for some Shakespeare plays, focusing on just four words. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] Each document is a single Shakespeare play. The columns tell us the counts of four words in those plays.\nWe can (partly) visualize the differences between these plays by looking at the occurrences of words. For example, the tragedies have a higher occurrence of battle and a lower occurrence of fool, while the comedies have more fool and less battle. (Source: Jurafsky and Martin, Speech and Language Processing 3rd edition, Chapter 6, page 112, Figure 6.4 link here)\nIf you take a look at that image, we can see that word counts can provide a notion of document similarity: More similar documents will have more similar word counts (at least for some important words) and dissimilar documents will have dissimilar word counts.\n\n\n\nThe most basic kind of term frequency is a word count. The term frequency of a word is just the number of times the term occurs in a document. \\[TF[d, t] = \\text{number of times term $t$ occurs in doc. $d$}\\]Each of the numbers in the document-term matrix is a term frequency. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] As we saw, term frequencies give us information about similarities and differences between documents. But this information is noisy for two reasons.\n\nProblem 1: The important words don’t stick out enough.\nProblem 2: The useless words stick out too much.\n\n\n\n\nCertain words show up in only a few documents, and therefore provide more information than words that show up all over the place. These words help us discriminate between document types.\nInformation that helps us distinguish between data points is called discriminative, and information that does not is called non-discriminative.\nWe would like to give the discriminative words a bump up by scaling up their feature representation, and tamp down the importance of non-discriminative words by scaling down their feature representations.\n\n\n\nTo scale our feature representations so that the numbers representing discriminative words are relatively higher, and the numbers representing non-discriminative words are lower, we use the inverse document frequency.\nFirst, we look at the document frequency of each word. \\[DF[t] = \\text{number of documents containing term } t\\] Then we get the inverse document frequency (IDF) for that term. The IDF is the total number of documents \\(N\\) divided by the number of documents \\(t\\) occurs in (\\(DF[t]\\)). \\[ IDF[t] = \\frac{N}{DF[t]}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, N = \\text{total number of docs}\\] When we multiply the term frequencies by the inverse document frequencies, we get new numbers for each term. Compared to the raw term frequencies (the counts in the bag-of-words matrix) the new numbers are bigger when the document frequency is lower (more discriminative), and smaller when the term’s document frequency is higher (less discriminative).\n\\[\\begin{array}{cccc} \\text{term occurs in more docs (less discriminative)} & \\rightarrow & \\text{high } DF[t] & \\rightarrow & \\text{low } IDF[t] \\\\\n\\text{term occurs in fewer docs (more discriminative)} & \\rightarrow & \\text{low } DF[t] & \\rightarrow & \\text{high } IDF[t] \\\\\\end{array}\\] The TF-IDF score is just the product of the term frequency and the document frequency. \\[TFIDF[d,t] = TF[d,t]\\cdot IDF[t] = TF[d,t]\\cdot \\frac{N}{DF[t]}\\]\n\n\n\nSuppose we have 5 words and 5 documents. We can see that words like the or a show up very frequently, words like and or might show up somewhat frequently, and words like terrific show up very infrequently.\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 2 & 0 & 0 & 1 \\\\\n2 & 3 & 1 & 0 & 0 \\\\\n3 & 4 & 0 & 0 & 0 \\\\\n2 & 2 & 3 & 2 & 0 \\\\\n1 & 0 & 1 & 1 & 0 \\\\\n\\end{array}\\] Let’s calculate the document frequency and inverse document frequency for each word.\n\\[\\begin{array}{} & \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\nDF[t] & 5 & 4 & 3 & 2 & 1 \\\\\nIDF[t] & 1 & 5/4 & 5/3 & 5/2 & 5 \\\\\n\\end{array}\\]\nIf we scale the term frequencies by the inverse document frequencies, we end with a feature representation where words that appear in fewer documents get higher scores.\nThe first column the remains the same, because its IDF is 1. The last column gets scaled up the most, so terrific now has the TFIDF score of 5, even though its count was just 1\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 5/2  & 0 & 0 & 5 \\\\\n2 & 15/4 & 5/3 & 0 & 0 \\\\\n3 & 5    & 0 & 0 & 0 \\\\\n2 & 10/4 & 5 & 5 & 0 \\\\\n1 & 0    & 5/3 & 5/2 & 0 \\\\\n\\end{array}\\]\n\n\n\nNotice that in the example, the word terrific’s TFIDF score is \\(5\\times\\) higher than its count. If we had \\(N=1000\\) documents, and the word terrific only appeared once in one document, then IDF[terrific] would be \\(1000\\) even though its count is just \\(1\\)!\nThis is out of proportion. We want to scale up infrequent words, but not so much that they drown out all the other information.\nThe solution is to use logarithms. Instead of using the raw frequencies for TF and IDF, we log scale them. Here are the new definitions.\nThe raw counts are the numbers in the bag-of-words matrix.\n\\[count[d,t] = \\text{number of times term $t$ appears in document $d$}\\]\nThe term frequency is now 1 plus the log of the count. \\[TF[d,t] = 1 + \\log_{10}(count[d,t])\\] The document frequency is the number of documents a term occurs in, just as before. \\[DF[t] = \\text{number of documents term $t$ occurs in}\\] The inverse document frequency is now the log of the old document frequency (\\(N\\) over \\(DF[t]\\)]). \\[IDF[t] = \\log_{10}\\left(\\frac{N}{DF[t]}\\right)\\]\n\n\n\nNext time we’ll see a worked-out example of TFIDF with log-scaling. We’ll also start using Python to calculate the model outputs."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#content",
    "href": "pages/lecturenotes/day4.html#content",
    "title": "Day 4",
    "section": "",
    "text": "On 2/6/24, we reviewed the bag-of-words encoding for documents, and learned about a more interesting and useful encoding. The more useful one is called term frequency-inverse document frequency (tf-idf).\nIt is kind of like a bag-of-words encoding, but gives more weight to informative (discriminative) words, and less weight to non-informative (non-discriminative) words like the, and, etc, which are usually less helpful for classification."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#the-document-term-matrix",
    "href": "pages/lecturenotes/day4.html#the-document-term-matrix",
    "title": "Day 4",
    "section": "",
    "text": "When we were looking at the bag-of-words encoding, we learned how to encode a dataset as a matrix like this one. This is called a document-term matrix, because each row represents a document, and each column represents a term (a word).\nHere is a subset of a document-term matrix for some Shakespeare plays, focusing on just four words. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] Each document is a single Shakespeare play. The columns tell us the counts of four words in those plays.\nWe can (partly) visualize the differences between these plays by looking at the occurrences of words. For example, the tragedies have a higher occurrence of battle and a lower occurrence of fool, while the comedies have more fool and less battle. (Source: Jurafsky and Martin, Speech and Language Processing 3rd edition, Chapter 6, page 112, Figure 6.4 link here)\nIf you take a look at that image, we can see that word counts can provide a notion of document similarity: More similar documents will have more similar word counts (at least for some important words) and dissimilar documents will have dissimilar word counts."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#term-frequency",
    "href": "pages/lecturenotes/day4.html#term-frequency",
    "title": "Day 4",
    "section": "",
    "text": "The most basic kind of term frequency is a word count. The term frequency of a word is just the number of times the term occurs in a document. \\[TF[d, t] = \\text{number of times term $t$ occurs in doc. $d$}\\]Each of the numbers in the document-term matrix is a term frequency. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] As we saw, term frequencies give us information about similarities and differences between documents. But this information is noisy for two reasons.\n\nProblem 1: The important words don’t stick out enough.\nProblem 2: The useless words stick out too much."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#discriminative-words",
    "href": "pages/lecturenotes/day4.html#discriminative-words",
    "title": "Day 4",
    "section": "",
    "text": "Certain words show up in only a few documents, and therefore provide more information than words that show up all over the place. These words help us discriminate between document types.\nInformation that helps us distinguish between data points is called discriminative, and information that does not is called non-discriminative.\nWe would like to give the discriminative words a bump up by scaling up their feature representation, and tamp down the importance of non-discriminative words by scaling down their feature representations."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#inverse-document-frequency",
    "href": "pages/lecturenotes/day4.html#inverse-document-frequency",
    "title": "Day 4",
    "section": "",
    "text": "To scale our feature representations so that the numbers representing discriminative words are relatively higher, and the numbers representing non-discriminative words are lower, we use the inverse document frequency.\nFirst, we look at the document frequency of each word. \\[DF[t] = \\text{number of documents containing term } t\\] Then we get the inverse document frequency (IDF) for that term. The IDF is the total number of documents \\(N\\) divided by the number of documents \\(t\\) occurs in (\\(DF[t]\\)). \\[ IDF[t] = \\frac{N}{DF[t]}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, N = \\text{total number of docs}\\] When we multiply the term frequencies by the inverse document frequencies, we get new numbers for each term. Compared to the raw term frequencies (the counts in the bag-of-words matrix) the new numbers are bigger when the document frequency is lower (more discriminative), and smaller when the term’s document frequency is higher (less discriminative).\n\\[\\begin{array}{cccc} \\text{term occurs in more docs (less discriminative)} & \\rightarrow & \\text{high } DF[t] & \\rightarrow & \\text{low } IDF[t] \\\\\n\\text{term occurs in fewer docs (more discriminative)} & \\rightarrow & \\text{low } DF[t] & \\rightarrow & \\text{high } IDF[t] \\\\\\end{array}\\] The TF-IDF score is just the product of the term frequency and the document frequency. \\[TFIDF[d,t] = TF[d,t]\\cdot IDF[t] = TF[d,t]\\cdot \\frac{N}{DF[t]}\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#example",
    "href": "pages/lecturenotes/day4.html#example",
    "title": "Day 4",
    "section": "",
    "text": "Suppose we have 5 words and 5 documents. We can see that words like the or a show up very frequently, words like and or might show up somewhat frequently, and words like terrific show up very infrequently.\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 2 & 0 & 0 & 1 \\\\\n2 & 3 & 1 & 0 & 0 \\\\\n3 & 4 & 0 & 0 & 0 \\\\\n2 & 2 & 3 & 2 & 0 \\\\\n1 & 0 & 1 & 1 & 0 \\\\\n\\end{array}\\] Let’s calculate the document frequency and inverse document frequency for each word.\n\\[\\begin{array}{} & \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\nDF[t] & 5 & 4 & 3 & 2 & 1 \\\\\nIDF[t] & 1 & 5/4 & 5/3 & 5/2 & 5 \\\\\n\\end{array}\\]\nIf we scale the term frequencies by the inverse document frequencies, we end with a feature representation where words that appear in fewer documents get higher scores.\nThe first column the remains the same, because its IDF is 1. The last column gets scaled up the most, so terrific now has the TFIDF score of 5, even though its count was just 1\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 5/2  & 0 & 0 & 5 \\\\\n2 & 15/4 & 5/3 & 0 & 0 \\\\\n3 & 5    & 0 & 0 & 0 \\\\\n2 & 10/4 & 5 & 5 & 0 \\\\\n1 & 0    & 5/3 & 5/2 & 0 \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#log-scaling",
    "href": "pages/lecturenotes/day4.html#log-scaling",
    "title": "Day 4",
    "section": "",
    "text": "Notice that in the example, the word terrific’s TFIDF score is \\(5\\times\\) higher than its count. If we had \\(N=1000\\) documents, and the word terrific only appeared once in one document, then IDF[terrific] would be \\(1000\\) even though its count is just \\(1\\)!\nThis is out of proportion. We want to scale up infrequent words, but not so much that they drown out all the other information.\nThe solution is to use logarithms. Instead of using the raw frequencies for TF and IDF, we log scale them. Here are the new definitions.\nThe raw counts are the numbers in the bag-of-words matrix.\n\\[count[d,t] = \\text{number of times term $t$ appears in document $d$}\\]\nThe term frequency is now 1 plus the log of the count. \\[TF[d,t] = 1 + \\log_{10}(count[d,t])\\] The document frequency is the number of documents a term occurs in, just as before. \\[DF[t] = \\text{number of documents term $t$ occurs in}\\] The inverse document frequency is now the log of the old document frequency (\\(N\\) over \\(DF[t]\\)]). \\[IDF[t] = \\log_{10}\\left(\\frac{N}{DF[t]}\\right)\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#for-next-time",
    "href": "pages/lecturenotes/day4.html#for-next-time",
    "title": "Day 4",
    "section": "",
    "text": "Next time we’ll see a worked-out example of TFIDF with log-scaling. We’ll also start using Python to calculate the model outputs."
  },
  {
    "objectID": "pages/lecturenotes/day3.html",
    "href": "pages/lecturenotes/day3.html",
    "title": "Day 3",
    "section": "",
    "text": "Today we saw our first example of learned document embeddings, namely the bag-of-words feature representation for documents.\n\n\n\nIn our first version of the binary classifier, we used simple hand-crafted features to encode documents. Each document (movie review) was encoded as a row of three numbers: total word count, positive word count, and negative word count.\nThis is usually not enough. In most cases, it is better to learn a feature representation from the data itself. Today we saw the most basic possible way to do this with text data: bag-of-words encoding.\nA bag-of-words encoding for a document is a vector of counts. To create bag-of-words encodings, we have to start by finding each unique word in the dataset and giving it its own column.\n\n\n\nHere is an example:\n\nwhat a waste of time\nthat was a great time\nbad acting\na dull movie from a great director\n\nList of unique words: what, a, waste, of, time,that, was, great, acting, bad, dull, from, director\nWe have 4 reviews with 14 unique words. So we will end up with a \\(4 \\times 14\\) feature matrix \\(x\\).\n\\[\\begin{array}{cccccccccccccc}\n\\text{what} & \\text{a} & \\text{waste} & \\text{of} & \\text{time} & \\text{that} & \\text{was} & \\text{great}  & \\text{acting} & \\text{bad} & \\text{movie} & \\text{dull} & \\text{from} & \\text{director} \\\\\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\]\nWe will need to multiply this with a \\(14 \\times 1\\) weight matrix \\(w\\) in order to get a single number \\(z\\). (Let’s ignore the bias for now, assume it’s \\(0\\).) Let’s assume that most words are zero (no effect), \\(\\text{great}\\) is \\(+1\\), and \\(\\text{waste, bad, dull}\\) are each -1. \\[ \\begin{array}{cc} \\text{what} & 0 \\\\\n\\text{a} & 0 \\\\\n\\text{waste} & -1 \\\\\n\\text{of} & 0 \\\\\n\\text{time} & 0 \\\\\n\\text{that} & 0 \\\\\n\\text{was} & 0 \\\\\n\\text{great} & 1 \\\\\n\\text{acting} & 0 \\\\\n\\text{bad} & -1 \\\\\n\\text{movie} & 0 \\\\\n\\text{dull} & -1 \\\\\n\\text{from} & 0 \\\\\n\\text{director} & 0 \\\\\n\\end{array}\\] One important point: These weights are chosen for sentiment analysis specifically. If we were doing a different kind of classification task (for example, trying to classify texts by topic) the weights would look totally different.\n\n\n\nNow let’s do the forward pass.\n\\[z = xw = \\left[\\begin{array}{cccccccccccccc}\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\right]\\left[\\begin{array}{c} 0 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\end{array}\\right]\\]\\[= \\left[\\begin{array}{c} 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\\n0 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0  \\\\ 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\  0 \\cdot 0 + 2 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0 \\\\ \\end{array}\\right]\\] \\[= \\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\] Our predictions are\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} 1/(1+e) \\\\ 1/(1+(1/e)) \\\\ 1/(1+e) \\\\ 1/(1+e) \\end{array}\\right] = \\left[\\begin{array}{c} 0.27 \\\\ 0.73 \\\\ 0.27 \\\\ 0.27 \\end{array}\\right]\\]\n\n\n\nThis is the simplest possible setup that lets us encode documents using features that are learned from the data (not just hand-crafted). Although bag-of-words representations can work ok in some situations, there are much better ways to learn embeddings.\nNext week, we’ll look an approach that is a step up from this: term frequency-inverse document frequency (tf-idf) encoding.\nLater on, we’ll learn an even better method: pre-trained word embeddings (e.g. word2vec), where a specialized neural network is used to learn vector representations for every word in the vocabulary."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#content",
    "href": "pages/lecturenotes/day3.html#content",
    "title": "Day 3",
    "section": "",
    "text": "Today we saw our first example of learned document embeddings, namely the bag-of-words feature representation for documents."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#learning-features-from-data",
    "href": "pages/lecturenotes/day3.html#learning-features-from-data",
    "title": "Day 3",
    "section": "",
    "text": "In our first version of the binary classifier, we used simple hand-crafted features to encode documents. Each document (movie review) was encoded as a row of three numbers: total word count, positive word count, and negative word count.\nThis is usually not enough. In most cases, it is better to learn a feature representation from the data itself. Today we saw the most basic possible way to do this with text data: bag-of-words encoding.\nA bag-of-words encoding for a document is a vector of counts. To create bag-of-words encodings, we have to start by finding each unique word in the dataset and giving it its own column."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#bag-of-words-example",
    "href": "pages/lecturenotes/day3.html#bag-of-words-example",
    "title": "Day 3",
    "section": "",
    "text": "Here is an example:\n\nwhat a waste of time\nthat was a great time\nbad acting\na dull movie from a great director\n\nList of unique words: what, a, waste, of, time,that, was, great, acting, bad, dull, from, director\nWe have 4 reviews with 14 unique words. So we will end up with a \\(4 \\times 14\\) feature matrix \\(x\\).\n\\[\\begin{array}{cccccccccccccc}\n\\text{what} & \\text{a} & \\text{waste} & \\text{of} & \\text{time} & \\text{that} & \\text{was} & \\text{great}  & \\text{acting} & \\text{bad} & \\text{movie} & \\text{dull} & \\text{from} & \\text{director} \\\\\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\]\nWe will need to multiply this with a \\(14 \\times 1\\) weight matrix \\(w\\) in order to get a single number \\(z\\). (Let’s ignore the bias for now, assume it’s \\(0\\).) Let’s assume that most words are zero (no effect), \\(\\text{great}\\) is \\(+1\\), and \\(\\text{waste, bad, dull}\\) are each -1. \\[ \\begin{array}{cc} \\text{what} & 0 \\\\\n\\text{a} & 0 \\\\\n\\text{waste} & -1 \\\\\n\\text{of} & 0 \\\\\n\\text{time} & 0 \\\\\n\\text{that} & 0 \\\\\n\\text{was} & 0 \\\\\n\\text{great} & 1 \\\\\n\\text{acting} & 0 \\\\\n\\text{bad} & -1 \\\\\n\\text{movie} & 0 \\\\\n\\text{dull} & -1 \\\\\n\\text{from} & 0 \\\\\n\\text{director} & 0 \\\\\n\\end{array}\\] One important point: These weights are chosen for sentiment analysis specifically. If we were doing a different kind of classification task (for example, trying to classify texts by topic) the weights would look totally different."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#forward-pass-calculating-model-predictions",
    "href": "pages/lecturenotes/day3.html#forward-pass-calculating-model-predictions",
    "title": "Day 3",
    "section": "",
    "text": "Now let’s do the forward pass.\n\\[z = xw = \\left[\\begin{array}{cccccccccccccc}\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\right]\\left[\\begin{array}{c} 0 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\end{array}\\right]\\]\\[= \\left[\\begin{array}{c} 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\\n0 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0  \\\\ 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\  0 \\cdot 0 + 2 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0 \\\\ \\end{array}\\right]\\] \\[= \\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\] Our predictions are\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} 1/(1+e) \\\\ 1/(1+(1/e)) \\\\ 1/(1+e) \\\\ 1/(1+e) \\end{array}\\right] = \\left[\\begin{array}{c} 0.27 \\\\ 0.73 \\\\ 0.27 \\\\ 0.27 \\end{array}\\right]\\]"
  },
  {
    "objectID": "pages/lecturenotes/day3.html#summing-up",
    "href": "pages/lecturenotes/day3.html#summing-up",
    "title": "Day 3",
    "section": "",
    "text": "This is the simplest possible setup that lets us encode documents using features that are learned from the data (not just hand-crafted). Although bag-of-words representations can work ok in some situations, there are much better ways to learn embeddings.\nNext week, we’ll look an approach that is a step up from this: term frequency-inverse document frequency (tf-idf) encoding.\nLater on, we’ll learn an even better method: pre-trained word embeddings (e.g. word2vec), where a specialized neural network is used to learn vector representations for every word in the vocabulary."
  }
]