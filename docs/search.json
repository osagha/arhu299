[
  {
    "objectID": "pages/lecturenotes/day3.html",
    "href": "pages/lecturenotes/day3.html",
    "title": "Day 3",
    "section": "",
    "text": "Today we saw our first example of learned document embeddings, namely the bag-of-words feature representation for documents.\n\n\n\nIn our first version of the binary classifier, we used simple hand-crafted features to encode documents. Each document (movie review) was encoded as a row of three numbers: total word count, positive word count, and negative word count.\nThis is usually not enough. In most cases, it is better to learn a feature representation from the data itself. Today we saw the most basic possible way to do this with text data: bag-of-words encoding.\nA bag-of-words encoding for a document is a vector of counts. To create bag-of-words encodings, we have to start by finding each unique word in the dataset and giving it its own column.\n\n\n\nHere is an example:\n\nwhat a waste of time\nthat was a great time\nbad acting\na dull movie from a great director\n\nList of unique words: what, a, waste, of, time,that, was, great, acting, bad, dull, from, director\nWe have 4 reviews with 14 unique words. So we will end up with a \\(4 \\times 14\\) feature matrix \\(x\\).\n\\[\\begin{array}{cccccccccccccc}\n\\text{what} & \\text{a} & \\text{waste} & \\text{of} & \\text{time} & \\text{that} & \\text{was} & \\text{great}  & \\text{acting} & \\text{bad} & \\text{movie} & \\text{dull} & \\text{from} & \\text{director} \\\\\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\]\nWe will need to multiply this with a \\(14 \\times 1\\) weight matrix \\(w\\) in order to get a single number \\(z\\). (Let’s ignore the bias for now, assume it’s \\(0\\).) Let’s assume that most words are zero (no effect), \\(\\text{great}\\) is \\(+1\\), and \\(\\text{waste, bad, dull}\\) are each -1. \\[ \\begin{array}{cc} \\text{what} & 0 \\\\\n\\text{a} & 0 \\\\\n\\text{waste} & -1 \\\\\n\\text{of} & 0 \\\\\n\\text{time} & 0 \\\\\n\\text{that} & 0 \\\\\n\\text{was} & 0 \\\\\n\\text{great} & 1 \\\\\n\\text{acting} & 0 \\\\\n\\text{bad} & -1 \\\\\n\\text{movie} & 0 \\\\\n\\text{dull} & -1 \\\\\n\\text{from} & 0 \\\\\n\\text{director} & 0 \\\\\n\\end{array}\\] One important point: These weights are chosen for sentiment analysis specifically. If we were doing a different kind of classification task (for example, trying to classify texts by topic) the weights would look totally different.\n\n\n\nNow let’s do the forward pass.\n\\[z = xw = \\left[\\begin{array}{cccccccccccccc}\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\right]\\left[\\begin{array}{c} 0 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\end{array}\\right]\\]\\[= \\left[\\begin{array}{c} 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\\n0 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0  \\\\ 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\  0 \\cdot 0 + 2 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0 \\\\ \\end{array}\\right]\\] \\[= \\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\] Our predictions are\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} 1/(1+e) \\\\ 1/(1+(1/e)) \\\\ 1/(1+e) \\\\ 1/(1+e) \\end{array}\\right] = \\left[\\begin{array}{c} 0.27 \\\\ 0.73 \\\\ 0.27 \\\\ 0.27 \\end{array}\\right]\\]\n\n\n\nThis is the simplest possible setup that lets us encode documents using features that are learned from the data (not just hand-crafted). Although bag-of-words representations can work ok in some situations, there are much better ways to learn embeddings.\nNext week, we’ll look an approach that is a step up from this: term frequency-inverse document frequency (tf-idf) encoding.\nLater on, we’ll learn an even better method: pre-trained word embeddings (e.g. word2vec), where a specialized neural network is used to learn vector representations for every word in the vocabulary."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#content",
    "href": "pages/lecturenotes/day3.html#content",
    "title": "Day 3",
    "section": "",
    "text": "Today we saw our first example of learned document embeddings, namely the bag-of-words feature representation for documents."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#learning-features-from-data",
    "href": "pages/lecturenotes/day3.html#learning-features-from-data",
    "title": "Day 3",
    "section": "",
    "text": "In our first version of the binary classifier, we used simple hand-crafted features to encode documents. Each document (movie review) was encoded as a row of three numbers: total word count, positive word count, and negative word count.\nThis is usually not enough. In most cases, it is better to learn a feature representation from the data itself. Today we saw the most basic possible way to do this with text data: bag-of-words encoding.\nA bag-of-words encoding for a document is a vector of counts. To create bag-of-words encodings, we have to start by finding each unique word in the dataset and giving it its own column."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#bag-of-words-example",
    "href": "pages/lecturenotes/day3.html#bag-of-words-example",
    "title": "Day 3",
    "section": "",
    "text": "Here is an example:\n\nwhat a waste of time\nthat was a great time\nbad acting\na dull movie from a great director\n\nList of unique words: what, a, waste, of, time,that, was, great, acting, bad, dull, from, director\nWe have 4 reviews with 14 unique words. So we will end up with a \\(4 \\times 14\\) feature matrix \\(x\\).\n\\[\\begin{array}{cccccccccccccc}\n\\text{what} & \\text{a} & \\text{waste} & \\text{of} & \\text{time} & \\text{that} & \\text{was} & \\text{great}  & \\text{acting} & \\text{bad} & \\text{movie} & \\text{dull} & \\text{from} & \\text{director} \\\\\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\]\nWe will need to multiply this with a \\(14 \\times 1\\) weight matrix \\(w\\) in order to get a single number \\(z\\). (Let’s ignore the bias for now, assume it’s \\(0\\).) Let’s assume that most words are zero (no effect), \\(\\text{great}\\) is \\(+1\\), and \\(\\text{waste, bad, dull}\\) are each -1. \\[ \\begin{array}{cc} \\text{what} & 0 \\\\\n\\text{a} & 0 \\\\\n\\text{waste} & -1 \\\\\n\\text{of} & 0 \\\\\n\\text{time} & 0 \\\\\n\\text{that} & 0 \\\\\n\\text{was} & 0 \\\\\n\\text{great} & 1 \\\\\n\\text{acting} & 0 \\\\\n\\text{bad} & -1 \\\\\n\\text{movie} & 0 \\\\\n\\text{dull} & -1 \\\\\n\\text{from} & 0 \\\\\n\\text{director} & 0 \\\\\n\\end{array}\\] One important point: These weights are chosen for sentiment analysis specifically. If we were doing a different kind of classification task (for example, trying to classify texts by topic) the weights would look totally different."
  },
  {
    "objectID": "pages/lecturenotes/day3.html#forward-pass-calculating-model-predictions",
    "href": "pages/lecturenotes/day3.html#forward-pass-calculating-model-predictions",
    "title": "Day 3",
    "section": "",
    "text": "Now let’s do the forward pass.\n\\[z = xw = \\left[\\begin{array}{cccccccccccccc}\n1 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\\right]\\left[\\begin{array}{c} 0 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n0 \\\\\n1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n-1 \\\\\n0 \\\\\n0 \\end{array}\\right]\\]\\[= \\left[\\begin{array}{c} 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\\n0 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0  \\\\ 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 1 \\cdot 0+ 1 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 \\\\  0 \\cdot 0 + 2 \\cdot 0 + 0 \\cdot (-1) + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot (-1) + 1 \\cdot 0 + 1 \\cdot 0 \\\\ \\end{array}\\right]\\] \\[= \\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\] Our predictions are\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} -1 \\\\ 1 \\\\ -1 \\\\ -1 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} 1/(1+e) \\\\ 1/(1+(1/e)) \\\\ 1/(1+e) \\\\ 1/(1+e) \\end{array}\\right] = \\left[\\begin{array}{c} 0.27 \\\\ 0.73 \\\\ 0.27 \\\\ 0.27 \\end{array}\\right]\\]"
  },
  {
    "objectID": "pages/lecturenotes/day3.html#summing-up",
    "href": "pages/lecturenotes/day3.html#summing-up",
    "title": "Day 3",
    "section": "",
    "text": "This is the simplest possible setup that lets us encode documents using features that are learned from the data (not just hand-crafted). Although bag-of-words representations can work ok in some situations, there are much better ways to learn embeddings.\nNext week, we’ll look an approach that is a step up from this: term frequency-inverse document frequency (tf-idf) encoding.\nLater on, we’ll learn an even better method: pre-trained word embeddings (e.g. word2vec), where a specialized neural network is used to learn vector representations for every word in the vocabulary."
  },
  {
    "objectID": "pages/lecturenotes/09-03.html",
    "href": "pages/lecturenotes/09-03.html",
    "title": "09-03",
    "section": "",
    "text": "A neural network is a kind of function. It can take inputs in array format, and return outputs in a (possibly different) array format.\nNeural networks can take many kinds of inputs, like text, images, or sound files. In each case, the input has to be encoded as an array of numbers.\n\n\n\n\nflowchart LR\n    A(Text) --&gt; Z{Neural network}\n    B(Image) --&gt; Z\n    C(Sound file) --&gt; Z\n    Z --&gt; D(\"Label (Classification)\")\n    Z --&gt; E(\"Text, Image, Sound (Generation)\")\n    \n\n\n\n\n\nIn the case of classification, the output is a label. In the case of generation, the output could be of many types, including more text, images, or sound.\n\n\n\nOn a computer, all text is an encoded sequence of characters. A character is a symbol like a letter or number, punctuation, spaces/tabs/newlines, or special characters like “?”” or emojis.\nUnicode is the global standard for text encoding. It assigns every character to a code point. For example, capital A is U+0041, lowercase a is U+0061, and the number 1 is U+0031. Unicode covers all symbols in all scripts, plus many other characters. Some other examples are below.\n\n\n\nSymbol\nCode point\n\n\n\n\n⽔\nU+2F54\n\n\n(Space)\nU+0020\n\n\nअ\nU+0905\n\n\n?\nU+003F\n\n\n\nYou can search for any symbol you like here and find its Unicode code point.\nCode points are turned into machine-readable representations by particular encoding schemes, which differ in how much memory they use. The most popular is UTF-8 (read more about this on the Wiki link).\n\n\n\nLet’s take text classification as an example. In class we talked about sentiment analysis, but classification is used for other purposes too, like spam detection or topic modeling.\n\n\n\n\nflowchart TD\n    A(Text Classification) --- B(Sentiment analysis)\n    A --- C(Spam detection)\n    A --- D(Topic modeling)\n    A --- E(...)\n\n\n\n\n\nIn text classification, the input is a string (a sequence of characters). Let’s take sentiment analysis as an example.\n\n\n\n\nflowchart LR\n    A(\"Text string &lt;br&gt; `This movie was great`\") --&gt; Z{Neural network}\n    Z --&gt; D(\"Positive ✓\")\n    Z --&gt; E(\"Negative ✗\")\n\n\n\n\n\nIn sentiment analysis, a model takes in a string and outputs a label, either positive or negative.\nIn practice, the model is rarely 100% sure about the label, so it outputs probabilities. For example, if 'This movie was great.' is the input, the model might output 0.9 for positive and 0.1 for negative.\nLater on, we’ll see how to get our model to output probabilities. For now, let’s just think of our model as assigning scores to documents, where a higher score means the document is more positive, and a lower score means the document is more negative.\n\n\nDocuments (like movie reviews) are just sequences of characters, as far as the computer is concerned. It doesn’t know that dog is more similar in meaning to canine than it is to philosophy—all it sees are the characters.\nWe need to extract meaningful information from these text documents, and put it in a format that the classification algorithm can use. This is called a feature representation of a document, and it takes the form of a vector of numbers. These numbers can be thought of as coordinates for points in a high-dimensional space.\nNo matter how many dimensions a vector space has, we can always calculate the distance between points in the space.\nIn class, we saw some examples of features that might be useful for sentiment analysis: the number of positive words, the number of negative words, and the number of total words (the length of the document).\n\n\n\nRemember that the goal is to assign each document a sentiment score. Let’s call the score \\(z\\). Once we extract a feature representation (a vector \\(x_0, x_1, x_2,...\\)) from our document, we need to decide what effect each feature will have on that document’s sentiment score \\(z\\). The effect of each feature is captured by a number in our model that we call a weight.\nEach feature has its own weight. (Notation: \\(w_0\\) is the weight for feature \\(x_0\\), \\(w_1\\) is the weight for \\(x_1\\), and so on.)\nThe sentiment score \\(z\\) of a document, which measures how positive the document is, can be summarized by writing \\(z\\) as a linear function of the features, where the weights are the coefficients.\n\\[z = w_0x_0 + w_1x_1 + w_2x_2 + ...\\]\nAs an example, suppose we use only three features: \\(x_0\\) is the positive word count, \\(x_1\\) is the negative word count, and \\(x_2\\) is the length. Now we set the weights. Let’s say that each positive word adds \\(1\\) to the score, each negative word subtracts \\(1\\) from the score, and each word (in the total count) subtracts \\(1/2\\). (There is nothing special about these numbers, it’s just an example.)\nIf we set the weights that way, then we would have \\(w_0 = 1, w_1 =-1, w_2 = -1/2\\). The score for a document would then look like\n\\[z = x_0 - x_1 - \\frac12 x_2\\]\nNext time, we’ll use matrix notation to generalize this idea, and write down a model that can classify multiple documents at once."
  },
  {
    "objectID": "pages/lecturenotes/09-03.html#what-is-a-neural-network-conceptually",
    "href": "pages/lecturenotes/09-03.html#what-is-a-neural-network-conceptually",
    "title": "09-03",
    "section": "",
    "text": "A neural network is a kind of function. It can take inputs in array format, and return outputs in a (possibly different) array format.\nNeural networks can take many kinds of inputs, like text, images, or sound files. In each case, the input has to be encoded as an array of numbers.\n\n\n\n\nflowchart LR\n    A(Text) --&gt; Z{Neural network}\n    B(Image) --&gt; Z\n    C(Sound file) --&gt; Z\n    Z --&gt; D(\"Label (Classification)\")\n    Z --&gt; E(\"Text, Image, Sound (Generation)\")\n    \n\n\n\n\n\nIn the case of classification, the output is a label. In the case of generation, the output could be of many types, including more text, images, or sound."
  },
  {
    "objectID": "pages/lecturenotes/09-03.html#what-is-text",
    "href": "pages/lecturenotes/09-03.html#what-is-text",
    "title": "09-03",
    "section": "",
    "text": "On a computer, all text is an encoded sequence of characters. A character is a symbol like a letter or number, punctuation, spaces/tabs/newlines, or special characters like “?”” or emojis.\nUnicode is the global standard for text encoding. It assigns every character to a code point. For example, capital A is U+0041, lowercase a is U+0061, and the number 1 is U+0031. Unicode covers all symbols in all scripts, plus many other characters. Some other examples are below.\n\n\n\nSymbol\nCode point\n\n\n\n\n⽔\nU+2F54\n\n\n(Space)\nU+0020\n\n\nअ\nU+0905\n\n\n?\nU+003F\n\n\n\nYou can search for any symbol you like here and find its Unicode code point.\nCode points are turned into machine-readable representations by particular encoding schemes, which differ in how much memory they use. The most popular is UTF-8 (read more about this on the Wiki link)."
  },
  {
    "objectID": "pages/lecturenotes/09-03.html#text-classification-example",
    "href": "pages/lecturenotes/09-03.html#text-classification-example",
    "title": "09-03",
    "section": "",
    "text": "Let’s take text classification as an example. In class we talked about sentiment analysis, but classification is used for other purposes too, like spam detection or topic modeling.\n\n\n\n\nflowchart TD\n    A(Text Classification) --- B(Sentiment analysis)\n    A --- C(Spam detection)\n    A --- D(Topic modeling)\n    A --- E(...)\n\n\n\n\n\nIn text classification, the input is a string (a sequence of characters). Let’s take sentiment analysis as an example.\n\n\n\n\nflowchart LR\n    A(\"Text string &lt;br&gt; `This movie was great`\") --&gt; Z{Neural network}\n    Z --&gt; D(\"Positive ✓\")\n    Z --&gt; E(\"Negative ✗\")\n\n\n\n\n\nIn sentiment analysis, a model takes in a string and outputs a label, either positive or negative.\nIn practice, the model is rarely 100% sure about the label, so it outputs probabilities. For example, if 'This movie was great.' is the input, the model might output 0.9 for positive and 0.1 for negative.\nLater on, we’ll see how to get our model to output probabilities. For now, let’s just think of our model as assigning scores to documents, where a higher score means the document is more positive, and a lower score means the document is more negative.\n\n\nDocuments (like movie reviews) are just sequences of characters, as far as the computer is concerned. It doesn’t know that dog is more similar in meaning to canine than it is to philosophy—all it sees are the characters.\nWe need to extract meaningful information from these text documents, and put it in a format that the classification algorithm can use. This is called a feature representation of a document, and it takes the form of a vector of numbers. These numbers can be thought of as coordinates for points in a high-dimensional space.\nNo matter how many dimensions a vector space has, we can always calculate the distance between points in the space.\nIn class, we saw some examples of features that might be useful for sentiment analysis: the number of positive words, the number of negative words, and the number of total words (the length of the document).\n\n\n\nRemember that the goal is to assign each document a sentiment score. Let’s call the score \\(z\\). Once we extract a feature representation (a vector \\(x_0, x_1, x_2,...\\)) from our document, we need to decide what effect each feature will have on that document’s sentiment score \\(z\\). The effect of each feature is captured by a number in our model that we call a weight.\nEach feature has its own weight. (Notation: \\(w_0\\) is the weight for feature \\(x_0\\), \\(w_1\\) is the weight for \\(x_1\\), and so on.)\nThe sentiment score \\(z\\) of a document, which measures how positive the document is, can be summarized by writing \\(z\\) as a linear function of the features, where the weights are the coefficients.\n\\[z = w_0x_0 + w_1x_1 + w_2x_2 + ...\\]\nAs an example, suppose we use only three features: \\(x_0\\) is the positive word count, \\(x_1\\) is the negative word count, and \\(x_2\\) is the length. Now we set the weights. Let’s say that each positive word adds \\(1\\) to the score, each negative word subtracts \\(1\\) from the score, and each word (in the total count) subtracts \\(1/2\\). (There is nothing special about these numbers, it’s just an example.)\nIf we set the weights that way, then we would have \\(w_0 = 1, w_1 =-1, w_2 = -1/2\\). The score for a document would then look like\n\\[z = x_0 - x_1 - \\frac12 x_2\\]\nNext time, we’ll use matrix notation to generalize this idea, and write down a model that can classify multiple documents at once."
  },
  {
    "objectID": "pages/lecturenotes/oldindex.html",
    "href": "pages/lecturenotes/oldindex.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "Lecture Notes\n\nDay 1: Intro, Text Encoding, Features, Classification\n\n\nDay 2: Binary classification with hand-crafted features\n\n\nDay 3: Learning bag-of-words features\n\n\nDay 4: TFIDF encoding, part 1\n\n\n2/8 - Day 5 - Notebook 0: Intro to Python\n\n\n2/13 - Day 6 - Notebook 1 More Python basics\n\n\n2/15 - Day 7 - Notebook 2 Intro to PyTorch\n\n\n2/20 - Day 8 - Notebook 3 - Lecture Notes\n\n\n3/7 - Notebook 4\n\n\n3/14 - Notebook 5\n\n\n3/26 - see Notebook 5\n\n\n3/28 - Language Models - see Chapter 3 of Jurafsky and Martin Speech and Language Processing (full book here)\n\n\n4/2 - Language Models (theory) continued\n\n\n4/4-11 - n-gram models in Python Notebook 6\n\n\n4/16-25 - transformers: word embeddings, positional embeddings, and attention - see Chapter 10\n\nMore resources on transformer language models:\n3blue1brown videos on transformers: part 1, on GPT,\npart 2, on attention\nAndrej Karpathy’s GPT from scratch"
  },
  {
    "objectID": "pages/lecturenotes/day8.html",
    "href": "pages/lecturenotes/day8.html",
    "title": "Day 8",
    "section": "",
    "text": "Today we introduced multi-class classification.\n\n\n\nSo far, we have been mostly concerned with binary classification. In binary classification, we produce an \\(N \\times 1\\) score vector \\(z\\). \\[z = xw + b\\]Then the sigmoid function is used to get predictions. \\[\\hat y = \\sigma(z)\\] Remember that sigmoid is defined as: \\[\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\] We end up with a vector of numbers between 0 and 1. The first number is interpreted as the probability that the first document is in class 1, the second number is the probability that the second document is in class 1, and so on.\nThat setup assumes that we have two classes, 0 and 1. What if we have more than two classes? Well, then we will need more than one number for each input document.\n\n\n\nSpecifically, if we have 4 classes, then we need four numbers for each document. The relative sizes of the four numbers will tell us how likely each class is to apply to the document.\nFor example, here is a small subset of the AG News dataset. We have four classes: politics (1), sports (2), science/tech (3), and business (4).\nWe want to build a model that tells us how likely each class is for each document."
  },
  {
    "objectID": "pages/lecturenotes/day8.html#content",
    "href": "pages/lecturenotes/day8.html#content",
    "title": "Day 8",
    "section": "",
    "text": "Today we introduced multi-class classification."
  },
  {
    "objectID": "pages/lecturenotes/day8.html#binary-classification",
    "href": "pages/lecturenotes/day8.html#binary-classification",
    "title": "Day 8",
    "section": "",
    "text": "So far, we have been mostly concerned with binary classification. In binary classification, we produce an \\(N \\times 1\\) score vector \\(z\\). \\[z = xw + b\\]Then the sigmoid function is used to get predictions. \\[\\hat y = \\sigma(z)\\] Remember that sigmoid is defined as: \\[\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\] We end up with a vector of numbers between 0 and 1. The first number is interpreted as the probability that the first document is in class 1, the second number is the probability that the second document is in class 1, and so on.\nThat setup assumes that we have two classes, 0 and 1. What if we have more than two classes? Well, then we will need more than one number for each input document."
  },
  {
    "objectID": "pages/lecturenotes/day8.html#multi-class-classification",
    "href": "pages/lecturenotes/day8.html#multi-class-classification",
    "title": "Day 8",
    "section": "",
    "text": "Specifically, if we have 4 classes, then we need four numbers for each document. The relative sizes of the four numbers will tell us how likely each class is to apply to the document.\nFor example, here is a small subset of the AG News dataset. We have four classes: politics (1), sports (2), science/tech (3), and business (4).\nWe want to build a model that tells us how likely each class is for each document."
  },
  {
    "objectID": "pages/homework/homework1.html",
    "href": "pages/homework/homework1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Due Tuesday September 17th.\nSolve each problem by hand. You may handwrite or type your solutions.\nIf you handwrite, your writing must be clearly legible.\n\n\nHere are some matrices of various shapes and sizes. \\[I_2 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, I_3 = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, P = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, Q = \\left[\\begin{array}{ccc} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{array}\\right]\\]\n\\[ V = \\left[\\begin{array}{ccc} 7 & 0 & -3  \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, W = \\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\\right] \\]\n\\[ A = \\left[\\begin{array}{ccc} -2 & 3 & 8 \\\\ 0 & 11 & -3 \\\\ -2 & 0 & 0 \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, B = \\left[\\begin{array}{ccc} 1 & 1 & -4 \\\\ -1 & 0 & -2  \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, C = \\left[\\begin{array}{cc} 0 & 3  \\\\ -1 & -5 \\\\ 5 & 7  \\end{array}\\right] \\]\n\nCalculate the matrix products \\(I_2B\\), \\(I_3A\\), \\(PA\\), and \\(QA\\).\nIn plain words, what do the matrices \\(I_2\\), \\(I_3\\), \\(P\\), and \\(Q\\) do to other matrices?\nCalculate the matrix products \\(VW\\) and \\(WV\\).\nWhat do you notice about \\(VW\\) and \\(WV\\)?\nThink about the products \\(AB\\), \\(BA\\), \\(AC\\), \\(CA\\), \\(BC\\),and \\(CB\\). Which of these products work?\nFor each product that works, calculate it.\n\n\n\n\nRead the lecture notes for September 5th (09-05) carefully, then do all of the following by hand.\n\nWrite down the weights and biases for the model.\nMake up five short movie reviews and count up their features (word count, positive word count, negative word count). Make sure you write down which words you consider positive and which words you consider negative!\nWrite down the \\(5 \\times 3\\) matrix \\(x\\) for your encoded data and a \\(5 \\times 1\\) column vector \\(y\\) for the true labels.\nUsing your own data matrix \\(x\\), along with the weights and biases from the Day 2 lecture notes, calculate the \\(z = xw + b\\).\nThen, calculate the model predictions \\(\\hat y = \\sigma(z)\\). (You can use a calculator for the sigmoid part.)\nCalculate the difference \\(y - \\hat y\\).\nHow did the model do on your data?\nCan you change the bias or the weights to improve the performance?"
  },
  {
    "objectID": "pages/homework/homework1.html#part-1-matrix-math-warmup",
    "href": "pages/homework/homework1.html#part-1-matrix-math-warmup",
    "title": "Homework 1",
    "section": "",
    "text": "Here are some matrices of various shapes and sizes. \\[I_2 = \\left[\\begin{array}{cc} 1 & 0 \\\\ 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, I_3 = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, P = \\left[\\begin{array}{ccc} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 1 \\end{array}\\right] \\,\\,\\,\\,\\, Q = \\left[\\begin{array}{ccc} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{array}\\right]\\]\n\\[ V = \\left[\\begin{array}{ccc} 7 & 0 & -3  \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\, W = \\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 3 \\end{array}\\right] \\]\n\\[ A = \\left[\\begin{array}{ccc} -2 & 3 & 8 \\\\ 0 & 11 & -3 \\\\ -2 & 0 & 0 \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, B = \\left[\\begin{array}{ccc} 1 & 1 & -4 \\\\ -1 & 0 & -2  \\end{array}\\right] \\,\\,\\,\\,\\, \\,\\,\\,\\,\\, C = \\left[\\begin{array}{cc} 0 & 3  \\\\ -1 & -5 \\\\ 5 & 7  \\end{array}\\right] \\]\n\nCalculate the matrix products \\(I_2B\\), \\(I_3A\\), \\(PA\\), and \\(QA\\).\nIn plain words, what do the matrices \\(I_2\\), \\(I_3\\), \\(P\\), and \\(Q\\) do to other matrices?\nCalculate the matrix products \\(VW\\) and \\(WV\\).\nWhat do you notice about \\(VW\\) and \\(WV\\)?\nThink about the products \\(AB\\), \\(BA\\), \\(AC\\), \\(CA\\), \\(BC\\),and \\(CB\\). Which of these products work?\nFor each product that works, calculate it."
  },
  {
    "objectID": "pages/homework/homework1.html#part-2-sentiment-analysis-case-study",
    "href": "pages/homework/homework1.html#part-2-sentiment-analysis-case-study",
    "title": "Homework 1",
    "section": "",
    "text": "Read the lecture notes for September 5th (09-05) carefully, then do all of the following by hand.\n\nWrite down the weights and biases for the model.\nMake up five short movie reviews and count up their features (word count, positive word count, negative word count). Make sure you write down which words you consider positive and which words you consider negative!\nWrite down the \\(5 \\times 3\\) matrix \\(x\\) for your encoded data and a \\(5 \\times 1\\) column vector \\(y\\) for the true labels.\nUsing your own data matrix \\(x\\), along with the weights and biases from the Day 2 lecture notes, calculate the \\(z = xw + b\\).\nThen, calculate the model predictions \\(\\hat y = \\sigma(z)\\). (You can use a calculator for the sigmoid part.)\nCalculate the difference \\(y - \\hat y\\).\nHow did the model do on your data?\nCan you change the bias or the weights to improve the performance?"
  },
  {
    "objectID": "pages/homework/homework3.html",
    "href": "pages/homework/homework3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Write your own code! Do not just copy code from the lecture notes or from the internet.\nYou may look at online resources for help. But write everything yourself.\nIf you are serious about learning this stuff, you need to build muscle memory for writing code, just like you need muscle memory for playing the piano.\nInstructions: Do all this in a single colab notebook. Turn it in in .ipynb format. At each step, write down what you are doing. You can use comments, or you can use text cells in the notebook. But you must say what you are doing at each step. You will get no credit unless you accurately describe what each part of your code is doing.\nIn this homework, you will train a binary classifier using 4 slightly different feature representations. You will compare their performance by looking at the training loss.\n\n\nDownload the .csv file for the IMDB sentiment analysis data. Load it into Python using Pandas, and extract the text and labels.\nUse the CountVectorizer to create a feature representation. You may want to limit the number of features using max_features or min_df + max_df. Then use .toarray and torch.tensor to put the feature matrix into Pytorch tensor format.\n\n\n\nCode up a one-layer linear classifier. Feel free to follow the code from class, but don’t just copy it, type everyting yourself (for learning purposes).\nTrain the classifier for 100 epochs on your feature matrix. PyTorch will tell you the loss after every 10 epochs. Write down the losses!\n\n\n\nNow, take your original feature matrix and normalize the rows: For each row, calculate the mean and standard deviation of the row. Subtract the mean of the row from every number in the row, and divide every number in the row by the standard deviation.\nUse the new normalized feature matrix, train the classifier again. Write down the losses. Compare the new losses to the losses from the first run. Did it improve?\n\n\n\nNow, build a new feature matrix using the TfidfVectorizer from sklearn.feature_extraction.text. This matrix won’t be a matrix of counts, it will be a TFIDF matrix like the ones we built by hand in class.\nIf you used the parameters max_features, max_df, or mid_df for the count vectorizer, use the same parameter values for the TFIDF vectorizer too! (Otherwise you can’t really compare them.)\nTrain the same linear classifier on the TFIDF matrix. Write down the losses again. How do these compare to the first two times?\n\n\n\nNow take the TFIDF feature matrix from Part 4 and normalize the rows. (Subtract the mean and divide by the standard deviation for each row, like in Part 3.)\nTrain the model on the normalized TFIDF matrix. How do the losses compare?"
  },
  {
    "objectID": "pages/homework/homework3.html#tf-idf-in-python",
    "href": "pages/homework/homework3.html#tf-idf-in-python",
    "title": "Homework 3",
    "section": "",
    "text": "Write your own code! Do not just copy code from the lecture notes or from the internet.\nYou may look at online resources for help. But write everything yourself.\nIf you are serious about learning this stuff, you need to build muscle memory for writing code, just like you need muscle memory for playing the piano.\nInstructions: Do all this in a single colab notebook. Turn it in in .ipynb format. At each step, write down what you are doing. You can use comments, or you can use text cells in the notebook. But you must say what you are doing at each step. You will get no credit unless you accurately describe what each part of your code is doing.\nIn this homework, you will train a binary classifier using 4 slightly different feature representations. You will compare their performance by looking at the training loss.\n\n\nDownload the .csv file for the IMDB sentiment analysis data. Load it into Python using Pandas, and extract the text and labels.\nUse the CountVectorizer to create a feature representation. You may want to limit the number of features using max_features or min_df + max_df. Then use .toarray and torch.tensor to put the feature matrix into Pytorch tensor format.\n\n\n\nCode up a one-layer linear classifier. Feel free to follow the code from class, but don’t just copy it, type everyting yourself (for learning purposes).\nTrain the classifier for 100 epochs on your feature matrix. PyTorch will tell you the loss after every 10 epochs. Write down the losses!\n\n\n\nNow, take your original feature matrix and normalize the rows: For each row, calculate the mean and standard deviation of the row. Subtract the mean of the row from every number in the row, and divide every number in the row by the standard deviation.\nUse the new normalized feature matrix, train the classifier again. Write down the losses. Compare the new losses to the losses from the first run. Did it improve?\n\n\n\nNow, build a new feature matrix using the TfidfVectorizer from sklearn.feature_extraction.text. This matrix won’t be a matrix of counts, it will be a TFIDF matrix like the ones we built by hand in class.\nIf you used the parameters max_features, max_df, or mid_df for the count vectorizer, use the same parameter values for the TFIDF vectorizer too! (Otherwise you can’t really compare them.)\nTrain the same linear classifier on the TFIDF matrix. Write down the losses again. How do these compare to the first two times?\n\n\n\nNow take the TFIDF feature matrix from Part 4 and normalize the rows. (Subtract the mean and divide by the standard deviation for each row, like in Part 3.)\nTrain the model on the normalized TFIDF matrix. How do the losses compare?"
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Resources",
    "section": "",
    "text": "I’ll be adding resources here throughout the semester.\n\n\n\n\nA tried-and-true standard textbook for computational linguistics and NLP. The book’s website includes slides too, which are great for a quick reference. Accompanying Youtube lectures can be found here.\n\n\n\nA high quality, detailed, and somewhat technical introduction to NLP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA brilliant visual introduction to deep learning by 3blue1brown (Grant Sanderson). See also his interactive courses on neural networks, linear algebra, and calculus.\n\n\n\n\n\n\nA Stanford NLP course by Dan Jurafsky, based on the textbook Speech and Language Processing linked above. More accessible than the other courses listed below.\n\n\n\nA Stanford course by Christopher Manning, free on Youtube. This is aimed at Computer Science majors, so it may be more accessible after you have already learned the basics in this course.\n\n\n\nA UC Berkeley course on computer vision, free on Youtube. Quite technical, and best approached after this class."
  },
  {
    "objectID": "pages/resources.html#textbooks",
    "href": "pages/resources.html#textbooks",
    "title": "Resources",
    "section": "",
    "text": "A tried-and-true standard textbook for computational linguistics and NLP. The book’s website includes slides too, which are great for a quick reference. Accompanying Youtube lectures can be found here.\n\n\n\nA high quality, detailed, and somewhat technical introduction to NLP."
  },
  {
    "objectID": "pages/resources.html#videos",
    "href": "pages/resources.html#videos",
    "title": "Resources",
    "section": "",
    "text": "A brilliant visual introduction to deep learning by 3blue1brown (Grant Sanderson). See also his interactive courses on neural networks, linear algebra, and calculus."
  },
  {
    "objectID": "pages/resources.html#full-courses",
    "href": "pages/resources.html#full-courses",
    "title": "Resources",
    "section": "",
    "text": "A Stanford NLP course by Dan Jurafsky, based on the textbook Speech and Language Processing linked above. More accessible than the other courses listed below.\n\n\n\nA Stanford course by Christopher Manning, free on Youtube. This is aimed at Computer Science majors, so it may be more accessible after you have already learned the basics in this course.\n\n\n\nA UC Berkeley course on computer vision, free on Youtube. Quite technical, and best approached after this class."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ARHU299",
    "section": "",
    "text": "This is the homepage for ARHU299: Machine Learning in Language and Art at the University of Maryland, College Park.\nPlease see the syllabus for more info."
  },
  {
    "objectID": "pages/syllabus.html",
    "href": "pages/syllabus.html",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Syllabus for ARHU299: Machine Learning in Language and Art at the University of Maryland, College Park.\nLast updated August 27, 2024.\nInstructor: Omar Agha (oagha@umd.edu)\nTA: Sathvik Nair (sathvik@umd.edu)\nOffice hours: TBD\n\n\nIn this course, you will learn how machine learning models read and write text, classify images, and more. This requires understanding some basic elements of linear algebra (vectors and matrices), calculus (the chain rule for derivatives), Python programming, and basic statistics. If you have not seen any of this before, you will learn the relevant tools along the way, in the context of solving applied problems, and with the aid of software that performs complex calculations for you.\nThis is a fast-paced class. There will be homework roughly every other week, and frequent in-class assignments. You will need to make a serious effort to learn about difficult technical topics. I assume you are here because you want to learn this material, and I will do everything I can to help you succeed.\nWe will work up from simple models to more complex ones. We will start with the simplest neural network architecture, the perceptron, and apply it to text classification tasks. After that, we will study deeper networks (multi layer perceptrons) that can be used for language modeling (predicting the next word).\nBy the end of the course, we will cover the basics of transformer models. Transformers are the building blocks of language models like GPT. Depending on the speed of your progress, we might also have time to discuss other neural network architectures like convolutional neural networks.\n\n\n\nEngineers and scientists who work on these topics usually do years of intense study in math and computer science before reaching proficiency. So it’s not realistic for us to go that deep in one semester.\nLuckily, building simple text and image models does not require as much expertise, only some Python skills and enough curiosity to figure things out on one’s own.\n\n\n\nYou will learn how to multiply matrices, use probability, and understand simple equations and graphs involving functions like \\(\\sin\\), \\(\\cos\\), and \\(\\log\\). You will see how these concepts are actually applied in real-world problems.\nMachine learning models use calculus (derivatives) under the hood, and you will learn the basics of how this works. But for the most part. we will rely on software to do those calculations for us.\n\n\n\nYou will learn how to use the Python programming language, along with some Python libraries. The most important libraries are PyTorch and Numpy, for matrix math, and Pandas, for data processing. We will also use Matplotlib, Seaborn, and Plotly for plotting, and Pillow for image processing.\nThe basic Python concepts you will learn are functions, classes, lists, tuples, dictionaries, for-loops, list comprehensions, and lambda expressions.\nIf you want to get a head start on your Python journey, google all the unfamiliar terms in this section and read up.\n\n\n\nThere is no textbook, since I haven’t found one that fits the audience of this course. I will post optional resources (books, articles, and videos) in the lecture notes, and under the Resources section of the course website.\n\n\n\nI will post lecture notes within 48 hours after the relevant lecture. Lecture notes will usually not be available before class, so you must pay attention and take your own notes during class sessions.\nAll lecture notes will be posted under Lecture Notes. I will also post code in the form of Python Notebooks on the website. You can open a copy in Google Colab to study and play with the code.\n\n\n\n\n\nI will assume that you are here because you want to learn. This applies to all policies and requirements: I assume that you will come to class and complete the homework because you want to learn.\nI will do my best to not waste your time, and to not impose any requirements on you that are unnecessary for your educational goals.\n\n\n\nAttendance is not directly graded. However, there will be frequent in-class assignments, and participation is a part of your grade. Also, missing class will make it hard to keep up. For this reason, poor attendance will generally lower your grade and prevent you from learning effectively.\n\n\n\nIn this class you will learn math, programming, and critical thinking skills by practicing them. Learning this material without practice is not possible.\nI find that accepting late homework usually causes more students to fall behind, ultimately harming their grades and progress, even if it feels generous in the short term. So, late homework will not be accepted unless I have already granted you an extension.\nWe will mostly be coding in Google Colab. If you want to use another editor you can, but your submitted code must be in .ipynb format and it must run correctly in Colab. (So, make sure to upload your file in Colab and test it there.)\n\n\n\nIf you need an extension on any assignment, contact me before noon on the due date, and verify that I approve. (Better to reach out earlier than that if possible.)\nIf you need to be excused from class, contact me before that class starts.\nIf you are excused from class, I will let you know if there are any in-class assignments that you need to make up.\n\n\n\nThe UMD Code of Academic Integrity defines plagiarism as “representing the words or ideas of another as one’s own in any academic course, exercise or research”. If you copy text or code from any source, you must treat is as a quotation and provide the source. (You can either link to the source or provide a full bibliographic citation.)\nPlagiarism on any assignment will result in a zero. Repeat plagiarists will be reported to the Office of Student Conduct.\nCopying work from another student is plagiarism, but group work is encouraged. All submitted work must be your own.\n\n\n\nGenerative “AI” tools like ChatGPT or Bing produce text in a random process by sampling from the most likely next words.\nThey can be useful in certain contexts. However, the randomly generated text is often biased and false in subtle ways, and plagiarized from other sources. (This is because the model’s next-word-probabilities are derived from documents scraped from the internet.)\nCopying and pasting the output of generative AI on any assignment will be considered plagiarism. (See the above definition.)\nIf you choose to use generative AI to help with solving problems, that’s ok, but you must ensure that all submitted work is your own, and you are responsible for any errors. (Using ChatGPT or other software to edit your prose is also ok, especially if you are not a native speaker of English.)\n\n\n\n\n\n\n\n50% homework\n20% quizzes\n10% participation\n20% final project\n\n\n\n\n\nBy the end of class, you will submit a 2-3 page project proposal. This can be for a product that you would like to create, or for a research project. In the proposal, please discuss in detail\n\nhow you would use the technologies we cover in class,\nwhat you think the most challenging components would be,\nwhat the likely social impact of the project would be.\n\nYou don’t need to turn in any working code. (You are welcome to do so, and you won’t be graded on the code, only the proposal.)\n\n\nBefore Oct 18, please schedule a meeting with me to talk through project ideas.\n\nOct 18: Turn in one paragraph on your project proposal. If you have multiple ideas, do a short paragraph for each.\nNov 15: Turn in a half page, more fleshed out version. Start to include some details about technologies, challenges, and/or social impact. (Worth 5% out of the full 20%.)\nDec 6: Final due date for project proposal.\n\n\n\n\n\n\n\nYour success is very important to me. I (or the TA) will answer your emails promptly (within 24 hours) and make time to meet with you if you need extra help.\nIf you need help, please contact me ASAP. Often, students fail because they wait too long to ask for help, and then fall too far behind to catch up. Don’t let this happen to you!\n\n\n\nIf you find yourself struggling to understand something, try re-reading the lecture notes or re-watching the assigned videos. Ideas often take multiple tries to click.\nI will link to helpful videos and articles in my lecture notes. Don’t forget to use them!\n\n\n\nGroup work is highly encouraged. Please try to find a study group early on, and try to meet up for each homework. As a reminder, please make sure that all submitted work is your own.\n\n\n\nTaking care of yourself is the foundation for all success in life. Make sure you come to class well-rested, nourished, and ready to learn. (If you need to eat or drink beverages in class, that’s fine with me.)\n\n\n\nIf mental health challenges are keeping you from doing your best, the university provides Counseling and Behavioral Health Services. The earlier you reach out, the more you can benefit from these resources.\n\n\n\n\nStudents with disabilities are very welcome in this class. If you require accommodations, please contact me as soon as possible. For more information about disability accommodations at UMD, please see the ADS webpage."
  },
  {
    "objectID": "pages/syllabus.html#course-summary",
    "href": "pages/syllabus.html#course-summary",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "In this course, you will learn how machine learning models read and write text, classify images, and more. This requires understanding some basic elements of linear algebra (vectors and matrices), calculus (the chain rule for derivatives), Python programming, and basic statistics. If you have not seen any of this before, you will learn the relevant tools along the way, in the context of solving applied problems, and with the aid of software that performs complex calculations for you.\nThis is a fast-paced class. There will be homework roughly every other week, and frequent in-class assignments. You will need to make a serious effort to learn about difficult technical topics. I assume you are here because you want to learn this material, and I will do everything I can to help you succeed.\nWe will work up from simple models to more complex ones. We will start with the simplest neural network architecture, the perceptron, and apply it to text classification tasks. After that, we will study deeper networks (multi layer perceptrons) that can be used for language modeling (predicting the next word).\nBy the end of the course, we will cover the basics of transformer models. Transformers are the building blocks of language models like GPT. Depending on the speed of your progress, we might also have time to discuss other neural network architectures like convolutional neural networks."
  },
  {
    "objectID": "pages/syllabus.html#how-deep-will-we-go",
    "href": "pages/syllabus.html#how-deep-will-we-go",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Engineers and scientists who work on these topics usually do years of intense study in math and computer science before reaching proficiency. So it’s not realistic for us to go that deep in one semester.\nLuckily, building simple text and image models does not require as much expertise, only some Python skills and enough curiosity to figure things out on one’s own."
  },
  {
    "objectID": "pages/syllabus.html#math-skills",
    "href": "pages/syllabus.html#math-skills",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "You will learn how to multiply matrices, use probability, and understand simple equations and graphs involving functions like \\(\\sin\\), \\(\\cos\\), and \\(\\log\\). You will see how these concepts are actually applied in real-world problems.\nMachine learning models use calculus (derivatives) under the hood, and you will learn the basics of how this works. But for the most part. we will rely on software to do those calculations for us."
  },
  {
    "objectID": "pages/syllabus.html#programming-skills",
    "href": "pages/syllabus.html#programming-skills",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "You will learn how to use the Python programming language, along with some Python libraries. The most important libraries are PyTorch and Numpy, for matrix math, and Pandas, for data processing. We will also use Matplotlib, Seaborn, and Plotly for plotting, and Pillow for image processing.\nThe basic Python concepts you will learn are functions, classes, lists, tuples, dictionaries, for-loops, list comprehensions, and lambda expressions.\nIf you want to get a head start on your Python journey, google all the unfamiliar terms in this section and read up."
  },
  {
    "objectID": "pages/syllabus.html#resources",
    "href": "pages/syllabus.html#resources",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "There is no textbook, since I haven’t found one that fits the audience of this course. I will post optional resources (books, articles, and videos) in the lecture notes, and under the Resources section of the course website."
  },
  {
    "objectID": "pages/syllabus.html#lecture-notes",
    "href": "pages/syllabus.html#lecture-notes",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "I will post lecture notes within 48 hours after the relevant lecture. Lecture notes will usually not be available before class, so you must pay attention and take your own notes during class sessions.\nAll lecture notes will be posted under Lecture Notes. I will also post code in the form of Python Notebooks on the website. You can open a copy in Google Colab to study and play with the code."
  },
  {
    "objectID": "pages/syllabus.html#class-policies",
    "href": "pages/syllabus.html#class-policies",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "I will assume that you are here because you want to learn. This applies to all policies and requirements: I assume that you will come to class and complete the homework because you want to learn.\nI will do my best to not waste your time, and to not impose any requirements on you that are unnecessary for your educational goals.\n\n\n\nAttendance is not directly graded. However, there will be frequent in-class assignments, and participation is a part of your grade. Also, missing class will make it hard to keep up. For this reason, poor attendance will generally lower your grade and prevent you from learning effectively.\n\n\n\nIn this class you will learn math, programming, and critical thinking skills by practicing them. Learning this material without practice is not possible.\nI find that accepting late homework usually causes more students to fall behind, ultimately harming their grades and progress, even if it feels generous in the short term. So, late homework will not be accepted unless I have already granted you an extension.\nWe will mostly be coding in Google Colab. If you want to use another editor you can, but your submitted code must be in .ipynb format and it must run correctly in Colab. (So, make sure to upload your file in Colab and test it there.)\n\n\n\nIf you need an extension on any assignment, contact me before noon on the due date, and verify that I approve. (Better to reach out earlier than that if possible.)\nIf you need to be excused from class, contact me before that class starts.\nIf you are excused from class, I will let you know if there are any in-class assignments that you need to make up.\n\n\n\nThe UMD Code of Academic Integrity defines plagiarism as “representing the words or ideas of another as one’s own in any academic course, exercise or research”. If you copy text or code from any source, you must treat is as a quotation and provide the source. (You can either link to the source or provide a full bibliographic citation.)\nPlagiarism on any assignment will result in a zero. Repeat plagiarists will be reported to the Office of Student Conduct.\nCopying work from another student is plagiarism, but group work is encouraged. All submitted work must be your own.\n\n\n\nGenerative “AI” tools like ChatGPT or Bing produce text in a random process by sampling from the most likely next words.\nThey can be useful in certain contexts. However, the randomly generated text is often biased and false in subtle ways, and plagiarized from other sources. (This is because the model’s next-word-probabilities are derived from documents scraped from the internet.)\nCopying and pasting the output of generative AI on any assignment will be considered plagiarism. (See the above definition.)\nIf you choose to use generative AI to help with solving problems, that’s ok, but you must ensure that all submitted work is your own, and you are responsible for any errors. (Using ChatGPT or other software to edit your prose is also ok, especially if you are not a native speaker of English.)"
  },
  {
    "objectID": "pages/syllabus.html#grades",
    "href": "pages/syllabus.html#grades",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "50% homework\n20% quizzes\n10% participation\n20% final project"
  },
  {
    "objectID": "pages/syllabus.html#project-proposal",
    "href": "pages/syllabus.html#project-proposal",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "By the end of class, you will submit a 2-3 page project proposal. This can be for a product that you would like to create, or for a research project. In the proposal, please discuss in detail\n\nhow you would use the technologies we cover in class,\nwhat you think the most challenging components would be,\nwhat the likely social impact of the project would be.\n\nYou don’t need to turn in any working code. (You are welcome to do so, and you won’t be graded on the code, only the proposal.)\n\n\nBefore Oct 18, please schedule a meeting with me to talk through project ideas.\n\nOct 18: Turn in one paragraph on your project proposal. If you have multiple ideas, do a short paragraph for each.\nNov 15: Turn in a half page, more fleshed out version. Start to include some details about technologies, challenges, and/or social impact. (Worth 5% out of the full 20%.)\nDec 6: Final due date for project proposal."
  },
  {
    "objectID": "pages/syllabus.html#how-to-succeed-in-this-class",
    "href": "pages/syllabus.html#how-to-succeed-in-this-class",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Your success is very important to me. I (or the TA) will answer your emails promptly (within 24 hours) and make time to meet with you if you need extra help.\nIf you need help, please contact me ASAP. Often, students fail because they wait too long to ask for help, and then fall too far behind to catch up. Don’t let this happen to you!\n\n\n\nIf you find yourself struggling to understand something, try re-reading the lecture notes or re-watching the assigned videos. Ideas often take multiple tries to click.\nI will link to helpful videos and articles in my lecture notes. Don’t forget to use them!\n\n\n\nGroup work is highly encouraged. Please try to find a study group early on, and try to meet up for each homework. As a reminder, please make sure that all submitted work is your own.\n\n\n\nTaking care of yourself is the foundation for all success in life. Make sure you come to class well-rested, nourished, and ready to learn. (If you need to eat or drink beverages in class, that’s fine with me.)\n\n\n\nIf mental health challenges are keeping you from doing your best, the university provides Counseling and Behavioral Health Services. The earlier you reach out, the more you can benefit from these resources."
  },
  {
    "objectID": "pages/syllabus.html#disability-accommodations",
    "href": "pages/syllabus.html#disability-accommodations",
    "title": "ARHU299 Syllabus",
    "section": "",
    "text": "Students with disabilities are very welcome in this class. If you require accommodations, please contact me as soon as possible. For more information about disability accommodations at UMD, please see the ADS webpage."
  },
  {
    "objectID": "pages/homework/homework2.html",
    "href": "pages/homework/homework2.html",
    "title": "Homework 2",
    "section": "",
    "text": "This homework has a by-hand component and a Python component. For the by-hand component, you can turn in a scan or a typed solution. For the Python component, turn in a .ipynb file."
  },
  {
    "objectID": "pages/homework/homework2.html#tf-idf-by-hand",
    "href": "pages/homework/homework2.html#tf-idf-by-hand",
    "title": "Homework 2",
    "section": "Tf-idf by hand",
    "text": "Tf-idf by hand\nMake up 5 short documents and construct the tf-idf matrix by hand for these 5 documents. Steps: 1. First make up your data. 2. Make the bag-of-words (document-term) matrix by counting the words in each document. 3. Then calculate the document frequency and inverse document frequency of each word. 4. Then divide each column by its IDF.\nUse the version without log-scaling:\n\\[TF[d,t] = count(d,t) \\,\\,\\,\\,\\,\\,\\,\\,\\, IDF[t] = \\frac{N}{DF[t]}\\] Show your work at each step!\nWhich words are the most discriminative words? (Meaning, which words appear in the fewest documents, and therefore provide the most information?)"
  },
  {
    "objectID": "pages/homework/homework2.html#python-warmup",
    "href": "pages/homework/homework2.html#python-warmup",
    "title": "Homework 2",
    "section": "Python warmup",
    "text": "Python warmup\n\nPart 1\nDefine the sigmoid function in Python. Use np.exp from the numpy library.\n\n# np is the standard abbreviation for numpy\nimport numpy as np\n\ndef sigmoid(z: float) -&gt; float:\n    # your code here...\n    return # ...\n\nWrite a function that takes a single string, splits it up on the spaces, and returns a set containing all the unique words. (Hint: Use the .split() method. You can turn a list x into a set using set(x).)\n\ndef split_into_set(string: str) -&gt; set[str]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = 'great movie'\ntest_output = set(['great', 'movie'])\nif split_into_set(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a list of strings and returns a set containing all the unique words for all the strings together. (This is the vocabulary construction step.) (Hint: You can use the previous function to help you.)\n\ndef get_word_set(list_of_strs: list[str]) -&gt; set[str]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = ['great movie', 'terrible movie']\ntest_output = set(['great', 'terrible', 'movie'])\nif get_word_set(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a single string and returns a dictionary whose keys are the unique words in the string, and whose values are the word counts. (Hint: use an if-else pattern inside a for-loop.)\n\ndef get_wc(string: str) -&gt; dict[str, int]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = 'great movie awesome movie'\ntest_output = {'great': 1, 'awesome': 1, 'movie': 2}\nif get_wc(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\nWrite a function that takes a list of strings and returns a dictionary whose keys are the unique words, and whose values are the word counts.\n\ndef get_wcs_from_list(list_of_strs: list[str]) -&gt; dict[str, int]:\n    # your code here...\n    return # ...\n\n# Use this to test your function\ntest_input = ['great movie', 'terrible movie']\ntest_output = {'great': 1, 'terrible': 1, 'movie': 2}\nif get_wcs_from_list(test_input) == test_output:\n    print('Test passed.')\nelse:\n    print('Test failed.')\n\nTest failed.\n\n\n\n\nPart 2\nWrite a function that takes a list of shapes and returns a list of randomly generated PyTorch tensors, where each tensor’s shape comes from the input list. (Hint: Each shape is a tuple of ints, so the input is a list of tuples of ints. Use torch.rand(...).)\n\nimport torch\n\ndef gen_tensors(shapes : list[tuple[int, int]]) -&gt; list[torch.Tensor]:\n    # your code here...\n    return # ...\n\n# check your output\ntest_input = [(1,1), (2,3), (5,4)]\nprint(gen_tensors(test_input))\n\nNone\n\n\nWrite a function that takes a list of strings and returns the bag-of-words matrix as a PyTorch tensor. (Hint: The numbers in the tensor are word counts. This is a multi-step process. Use the functions you wrote in Part 1 to help you.)\n\nimport torch\n\ndef get_bow_matrix(list_of_strs: list[str]) -&gt; torch.Tensor:\n    # your code here ...\n    return # ...\n\n# check your output here\ntest_input = [\n    'great movie',\n    'what crap',\n    'what a great movie',\n]\nprint(get_bow_matrix(test_input))\n\nNone"
  },
  {
    "objectID": "pages/homework/homework2.html#next-time",
    "href": "pages/homework/homework2.html#next-time",
    "title": "Homework 2",
    "section": "Next time",
    "text": "Next time\nNext homework, we’ll put the pieces together to do TFIDF in Python!"
  },
  {
    "objectID": "pages/homework/index.html",
    "href": "pages/homework/index.html",
    "title": "Homework",
    "section": "",
    "text": "Homework\n\nHomework 1: Matrix Multiplication and Text Classification"
  },
  {
    "objectID": "pages/lecturenotes/09-05.html",
    "href": "pages/lecturenotes/09-05.html",
    "title": "09-05",
    "section": "",
    "text": "Our goal is to sort movie reviews into two classes, positive and negative. The strategy has been to generate a score \\[z = xw + b\\] for each movie review, and tweak the formula behind the scores so that positive reviews have higher scores and negative reviews have lower scores.\nThe score \\(z\\) can be any real number. (Meaning, any number on the number line, from negative infinity to positive infinity.) But what we actually want is for the model to guess whether the review is positive or negative.\nTo do this, let’s assume that 1 means positive and 0 means negative. We want to take our scores, which go from negative to positive infinity, and “squish” them down into the interval (0,1).\nTo do this, we use the sigmoid function, which does exactly that. In addition to squishing the scores into a bounded interval, sigmoid has the property that \\(\\sigma(0) = \\frac12\\). When \\(z\\) gets very large in the positive direction, \\(\\sigma(z)\\) gets close to \\(1\\), and when \\(z\\) gets large in the negative direction, \\(\\sigma(z)\\) gets close to \\(0\\).\n\\[ \\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\]"
  },
  {
    "objectID": "pages/lecturenotes/09-05.html#model-setup",
    "href": "pages/lecturenotes/09-05.html#model-setup",
    "title": "09-05",
    "section": "",
    "text": "Our goal is to sort movie reviews into two classes, positive and negative. The strategy has been to generate a score \\[z = xw + b\\] for each movie review, and tweak the formula behind the scores so that positive reviews have higher scores and negative reviews have lower scores.\nThe score \\(z\\) can be any real number. (Meaning, any number on the number line, from negative infinity to positive infinity.) But what we actually want is for the model to guess whether the review is positive or negative.\nTo do this, let’s assume that 1 means positive and 0 means negative. We want to take our scores, which go from negative to positive infinity, and “squish” them down into the interval (0,1).\nTo do this, we use the sigmoid function, which does exactly that. In addition to squishing the scores into a bounded interval, sigmoid has the property that \\(\\sigma(0) = \\frac12\\). When \\(z\\) gets very large in the positive direction, \\(\\sigma(z)\\) gets close to \\(1\\), and when \\(z\\) gets large in the negative direction, \\(\\sigma(z)\\) gets close to \\(0\\).\n\\[ \\sigma(z) = \\frac{1}{1 + \\exp(-z)}\\]"
  },
  {
    "objectID": "pages/lecturenotes/09-05.html#from-scores-to-model-predictions",
    "href": "pages/lecturenotes/09-05.html#from-scores-to-model-predictions",
    "title": "09-05",
    "section": "From scores to model predictions",
    "text": "From scores to model predictions\nRemember that \\(z\\) is a vector of scores, with one score for every document we put in. For example, if we put in \\(5\\) documents, then \\(x\\) will have \\(5\\) rows, and \\(z\\) will also have \\(5\\) rows. Therefore, \\(z\\) would have only \\(5\\) numbers, since it has just one column.\nThe model’s predictions are called \\(\\hat y\\), which is pronounced “y hat”. We get the predictions by taking our scores and putting them through the sigmoid function, so \\(\\hat y = \\sigma(z)\\). Each score in \\(\\hat y\\) is a number between 0 and 1. We can interpret each of these numbers as the probability that the corresponding review is positive.\nNOTE: The symbol \\(\\sigma\\) is the greek letter sigma. The sigmoid function is also known as the logistic function."
  },
  {
    "objectID": "pages/lecturenotes/09-05.html#example",
    "href": "pages/lecturenotes/09-05.html#example",
    "title": "09-05",
    "section": "Example",
    "text": "Example\nFor our example, let’s use three features: the total word count of the document, the number of positive words, and the number of negative words. (Note that the order is a bit different from the one we used in class.) Here are four example documents.\n\n\n\n\nflowchart LR\n    W[\"+, I was happy with this great movie\"] --&gt; A[\"[6, 2, 0]\"]\n    X[\"- , That was terrible\"] --&gt; B[\"[3, 0, 1]\"]\n    Y[\"+, Great movie\"] --&gt; C[\"[2, 1, 0]\"]\n    Z[\"-, The terrible acting actually made it fun to watch\"] --&gt; D[\"[7, 1, 1]\"]"
  },
  {
    "objectID": "pages/lecturenotes/09-05.html#matrix-multiplication",
    "href": "pages/lecturenotes/09-05.html#matrix-multiplication",
    "title": "09-05",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nTo calculate the scores for multiple reviews at once, we use matrix multiplication. Each feature representation is a row of 3 numbers, so if we have 4 reviews in the dataset, then we end up with a \\(4\\times 3\\) matrix representing the dataset. \\(y\\) is a vector representing the true labels of each example, 1 for positive and 0 for negative.\n\\[y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, x = \\left[\\begin{array}{ccc} 6 & 2 & 0 \\\\ 3 & 0 & 1 \\\\ 2 & 1 & 0 \\\\ 7 & 1 & 1 \\end{array}\\right]\\]\nOur weights will be a \\(3 \\times 1\\) column because we have three features. The bias will be a single number. The bias is repeated 4 times in a column, so that it has the same shape as \\(xw\\) (\\(4 \\times 1\\)).\nHere’s an example weight matrix. Here we are giving weight \\(-1\\) to the total word count (length), weight \\(2\\) to the positive words, and weight \\(-2\\) to the negative words.\n\\[w = \\left[\\begin{array}{ccc} -1 \\\\ 2 \\\\ -2 \\end{array}\\right]\\]\nLet’s make our example bias \\(4\\). Then we end up with the following \\(z\\) scores.\n\\[z = xw + b = \\left[\\begin{array}{ccc} 6 & 2 & 0 \\\\ 3 & 0 & 1 \\\\ 2 & 1 & 0 \\\\ 7 & 1 & 1 \\end{array}\\right] \\left[\\begin{array}{ccc} -1 \\\\ 2 \\\\ -2 \\end{array}\\right] + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right]\\]\n\\[= \\left[\\begin{array}{ccc} 6 \\cdot (-1) + 2 \\cdot 2 + 0 \\cdot (-2) \\\\ 3 \\cdot (-1) + 0 \\cdot 2 + 1 \\cdot (-2) \\\\ 2 \\cdot (-1) + 1 \\cdot 2 + 0 \\cdot (-2) \\\\ 7 \\cdot (-1) + 1 \\cdot 2 + 1 \\cdot (-2) \\end{array}\\right]  + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right] = \\left[\\begin{array}{ccc} -2 \\\\ -5 \\\\ 0 \\\\ -7 \\end{array}\\right]  + \\left[\\begin{array}{ccc} 4 \\\\ 4 \\\\ 4 \\\\ 4 \\end{array}\\right] = \\left[\\begin{array}{ccc} 2 \\\\ -1 \\\\ 4 \\\\ -3 \\end{array}\\right]\\]\nWith these weights and biases, we end up with positive \\(z\\) for reviews 1 and 3, and negative \\(z\\) for reviews 2 and 4. The sigmoid \\(\\sigma\\) function will turn the positive \\(z\\) scores into probabilities greater than 0.5, and negative z scores into probabilities smaller than 0.5. Sigmoid takes in a single number, so we apply it element-wise (meaning, to each element of the vector).\n\\[\\hat y = \\sigma(z) = \\sigma\\left(\\left[\\begin{array}{c} 2 \\\\ -1 \\\\ 4 \\\\ -3 \\end{array}\\right]\\right) = \\left[\\begin{array}{c} \\sigma(2) \\\\ \\sigma(-1) \\\\ \\sigma(4) \\\\ \\sigma(-3) \\end{array}\\right] = \\left[\\begin{array}{c} 0.88 \\\\ 0.27 \\\\ 0.95 \\\\ 0.05 \\end{array}\\right]\\]\nRecall that the true labels for the documents were:\n\\[y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right]\\]\nWe can look at the differences between the true labels \\(y\\) and the predicted labels \\(\\hat y\\). The goal is to approximate the true labels as closely as possible, so the smaller these differences are, the better our model is.\n\\[y  - \\hat y = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right] - \\left[\\begin{array}{c} 0.88 \\\\ 0.27 \\\\ 0.95 \\\\ 0.05 \\end{array}\\right] \\,\\,\\,\\, = \\,\\,\\,\\, \\left[\\begin{array}{c} 0.12 \\\\ -0.27 \\\\ 0.05 \\\\ -0.05 \\end{array}\\right]\\]"
  },
  {
    "objectID": "pages/lecturenotes/09-05.html#summing-up",
    "href": "pages/lecturenotes/09-05.html#summing-up",
    "title": "09-05",
    "section": "Summing up",
    "text": "Summing up\nWe now know how to write down the formulas for one-layer sigmoid classifier and apply it to binary text classification. In the future, we’ll look multi-class classification (where there are more than two output labels).\nWe’ll also start to look at representation learning, the art of learning features from data (rather than hand-crafting them, like we’ve done here)."
  },
  {
    "objectID": "pages/lecturenotes/day4.html",
    "href": "pages/lecturenotes/day4.html",
    "title": "Day 4",
    "section": "",
    "text": "On 2/6/24, we reviewed the bag-of-words encoding for documents, and learned about a more interesting and useful encoding. The more useful one is called term frequency-inverse document frequency (tf-idf).\nIt is kind of like a bag-of-words encoding, but gives more weight to informative (discriminative) words, and less weight to non-informative (non-discriminative) words like the, and, etc, which are usually less helpful for classification.\n\n\n\nWhen we were looking at the bag-of-words encoding, we learned how to encode a dataset as a matrix like this one. This is called a document-term matrix, because each row represents a document, and each column represents a term (a word).\nHere is a subset of a document-term matrix for some Shakespeare plays, focusing on just four words. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] Each document is a single Shakespeare play. The columns tell us the counts of four words in those plays.\nWe can (partly) visualize the differences between these plays by looking at the occurrences of words. For example, the tragedies have a higher occurrence of battle and a lower occurrence of fool, while the comedies have more fool and less battle. (Source: Jurafsky and Martin, Speech and Language Processing 3rd edition, Chapter 6, page 112, Figure 6.4 link here)\nIf you take a look at that image, we can see that word counts can provide a notion of document similarity: More similar documents will have more similar word counts (at least for some important words) and dissimilar documents will have dissimilar word counts.\n\n\n\nThe most basic kind of term frequency is a word count. The term frequency of a word is just the number of times the term occurs in a document. \\[TF[d, t] = \\text{number of times term $t$ occurs in doc. $d$}\\]Each of the numbers in the document-term matrix is a term frequency. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] As we saw, term frequencies give us information about similarities and differences between documents. But this information is noisy for two reasons.\n\nProblem 1: The important words don’t stick out enough.\nProblem 2: The useless words stick out too much.\n\n\n\n\nCertain words show up in only a few documents, and therefore provide more information than words that show up all over the place. These words help us discriminate between document types.\nInformation that helps us distinguish between data points is called discriminative, and information that does not is called non-discriminative.\nWe would like to give the discriminative words a bump up by scaling up their feature representation, and tamp down the importance of non-discriminative words by scaling down their feature representations.\n\n\n\nTo scale our feature representations so that the numbers representing discriminative words are relatively higher, and the numbers representing non-discriminative words are lower, we use the inverse document frequency.\nFirst, we look at the document frequency of each word. \\[DF[t] = \\text{number of documents containing term } t\\] Then we get the inverse document frequency (IDF) for that term. The IDF is the total number of documents \\(N\\) divided by the number of documents \\(t\\) occurs in (\\(DF[t]\\)). \\[ IDF[t] = \\frac{N}{DF[t]}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, N = \\text{total number of docs}\\] When we multiply the term frequencies by the inverse document frequencies, we get new numbers for each term. Compared to the raw term frequencies (the counts in the bag-of-words matrix) the new numbers are bigger when the document frequency is lower (more discriminative), and smaller when the term’s document frequency is higher (less discriminative).\n\\[\\begin{array}{cccc} \\text{term occurs in more docs (less discriminative)} & \\rightarrow & \\text{high } DF[t] & \\rightarrow & \\text{low } IDF[t] \\\\\n\\text{term occurs in fewer docs (more discriminative)} & \\rightarrow & \\text{low } DF[t] & \\rightarrow & \\text{high } IDF[t] \\\\\\end{array}\\] The TF-IDF score is just the product of the term frequency and the document frequency. \\[TFIDF[d,t] = TF[d,t]\\cdot IDF[t] = TF[d,t]\\cdot \\frac{N}{DF[t]}\\]\n\n\n\nSuppose we have 5 words and 5 documents. We can see that words like the or a show up very frequently, words like and or might show up somewhat frequently, and words like terrific show up very infrequently.\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 2 & 0 & 0 & 1 \\\\\n2 & 3 & 1 & 0 & 0 \\\\\n3 & 4 & 0 & 0 & 0 \\\\\n2 & 2 & 3 & 2 & 0 \\\\\n1 & 0 & 1 & 1 & 0 \\\\\n\\end{array}\\] Let’s calculate the document frequency and inverse document frequency for each word.\n\\[\\begin{array}{} & \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\nDF[t] & 5 & 4 & 3 & 2 & 1 \\\\\nIDF[t] & 1 & 5/4 & 5/3 & 5/2 & 5 \\\\\n\\end{array}\\]\nIf we scale the term frequencies by the inverse document frequencies, we end with a feature representation where words that appear in fewer documents get higher scores.\nThe first column the remains the same, because its IDF is 1. The last column gets scaled up the most, so terrific now has the TFIDF score of 5, even though its count was just 1\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 5/2  & 0 & 0 & 5 \\\\\n2 & 15/4 & 5/3 & 0 & 0 \\\\\n3 & 5    & 0 & 0 & 0 \\\\\n2 & 10/4 & 5 & 5 & 0 \\\\\n1 & 0    & 5/3 & 5/2 & 0 \\\\\n\\end{array}\\]\n\n\n\nNotice that in the example, the word terrific’s TFIDF score is \\(5\\times\\) higher than its count. If we had \\(N=1000\\) documents, and the word terrific only appeared once in one document, then IDF[terrific] would be \\(1000\\) even though its count is just \\(1\\)!\nThis is out of proportion. We want to scale up infrequent words, but not so much that they drown out all the other information.\nThe solution is to use logarithms. Instead of using the raw frequencies for TF and IDF, we log scale them. Here are the new definitions.\nThe raw counts are the numbers in the bag-of-words matrix.\n\\[count[d,t] = \\text{number of times term $t$ appears in document $d$}\\]\nThe term frequency is now 1 plus the log of the count. \\[TF[d,t] = 1 + \\log_{10}(count[d,t])\\] The document frequency is the number of documents a term occurs in, just as before. \\[DF[t] = \\text{number of documents term $t$ occurs in}\\] The inverse document frequency is now the log of the old document frequency (\\(N\\) over \\(DF[t]\\)]). \\[IDF[t] = \\log_{10}\\left(\\frac{N}{DF[t]}\\right)\\]\n\n\n\nNext time we’ll see a worked-out example of TFIDF with log-scaling. We’ll also start using Python to calculate the model outputs."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#content",
    "href": "pages/lecturenotes/day4.html#content",
    "title": "Day 4",
    "section": "",
    "text": "On 2/6/24, we reviewed the bag-of-words encoding for documents, and learned about a more interesting and useful encoding. The more useful one is called term frequency-inverse document frequency (tf-idf).\nIt is kind of like a bag-of-words encoding, but gives more weight to informative (discriminative) words, and less weight to non-informative (non-discriminative) words like the, and, etc, which are usually less helpful for classification."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#the-document-term-matrix",
    "href": "pages/lecturenotes/day4.html#the-document-term-matrix",
    "title": "Day 4",
    "section": "",
    "text": "When we were looking at the bag-of-words encoding, we learned how to encode a dataset as a matrix like this one. This is called a document-term matrix, because each row represents a document, and each column represents a term (a word).\nHere is a subset of a document-term matrix for some Shakespeare plays, focusing on just four words. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] Each document is a single Shakespeare play. The columns tell us the counts of four words in those plays.\nWe can (partly) visualize the differences between these plays by looking at the occurrences of words. For example, the tragedies have a higher occurrence of battle and a lower occurrence of fool, while the comedies have more fool and less battle. (Source: Jurafsky and Martin, Speech and Language Processing 3rd edition, Chapter 6, page 112, Figure 6.4 link here)\nIf you take a look at that image, we can see that word counts can provide a notion of document similarity: More similar documents will have more similar word counts (at least for some important words) and dissimilar documents will have dissimilar word counts."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#term-frequency",
    "href": "pages/lecturenotes/day4.html#term-frequency",
    "title": "Day 4",
    "section": "",
    "text": "The most basic kind of term frequency is a word count. The term frequency of a word is just the number of times the term occurs in a document. \\[TF[d, t] = \\text{number of times term $t$ occurs in doc. $d$}\\]Each of the numbers in the document-term matrix is a term frequency. \\[\\begin{array}{ccccc} & battle & good & fool & wit \\\\\n\\text{As You Like It} & 1 & 114 & 36 & 20 \\\\\n\\text{Twelfth Night} & 0 & 80 & 58 & 15 \\\\\n\\text{Julius Caesar} & 7 & 62 & 1 & 2 \\\\\n\\text{Henry V} & 12 & 89 & 4 & 3\\end{array}\\] As we saw, term frequencies give us information about similarities and differences between documents. But this information is noisy for two reasons.\n\nProblem 1: The important words don’t stick out enough.\nProblem 2: The useless words stick out too much."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#discriminative-words",
    "href": "pages/lecturenotes/day4.html#discriminative-words",
    "title": "Day 4",
    "section": "",
    "text": "Certain words show up in only a few documents, and therefore provide more information than words that show up all over the place. These words help us discriminate between document types.\nInformation that helps us distinguish between data points is called discriminative, and information that does not is called non-discriminative.\nWe would like to give the discriminative words a bump up by scaling up their feature representation, and tamp down the importance of non-discriminative words by scaling down their feature representations."
  },
  {
    "objectID": "pages/lecturenotes/day4.html#inverse-document-frequency",
    "href": "pages/lecturenotes/day4.html#inverse-document-frequency",
    "title": "Day 4",
    "section": "",
    "text": "To scale our feature representations so that the numbers representing discriminative words are relatively higher, and the numbers representing non-discriminative words are lower, we use the inverse document frequency.\nFirst, we look at the document frequency of each word. \\[DF[t] = \\text{number of documents containing term } t\\] Then we get the inverse document frequency (IDF) for that term. The IDF is the total number of documents \\(N\\) divided by the number of documents \\(t\\) occurs in (\\(DF[t]\\)). \\[ IDF[t] = \\frac{N}{DF[t]}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, N = \\text{total number of docs}\\] When we multiply the term frequencies by the inverse document frequencies, we get new numbers for each term. Compared to the raw term frequencies (the counts in the bag-of-words matrix) the new numbers are bigger when the document frequency is lower (more discriminative), and smaller when the term’s document frequency is higher (less discriminative).\n\\[\\begin{array}{cccc} \\text{term occurs in more docs (less discriminative)} & \\rightarrow & \\text{high } DF[t] & \\rightarrow & \\text{low } IDF[t] \\\\\n\\text{term occurs in fewer docs (more discriminative)} & \\rightarrow & \\text{low } DF[t] & \\rightarrow & \\text{high } IDF[t] \\\\\\end{array}\\] The TF-IDF score is just the product of the term frequency and the document frequency. \\[TFIDF[d,t] = TF[d,t]\\cdot IDF[t] = TF[d,t]\\cdot \\frac{N}{DF[t]}\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#example",
    "href": "pages/lecturenotes/day4.html#example",
    "title": "Day 4",
    "section": "",
    "text": "Suppose we have 5 words and 5 documents. We can see that words like the or a show up very frequently, words like and or might show up somewhat frequently, and words like terrific show up very infrequently.\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 2 & 0 & 0 & 1 \\\\\n2 & 3 & 1 & 0 & 0 \\\\\n3 & 4 & 0 & 0 & 0 \\\\\n2 & 2 & 3 & 2 & 0 \\\\\n1 & 0 & 1 & 1 & 0 \\\\\n\\end{array}\\] Let’s calculate the document frequency and inverse document frequency for each word.\n\\[\\begin{array}{} & \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\nDF[t] & 5 & 4 & 3 & 2 & 1 \\\\\nIDF[t] & 1 & 5/4 & 5/3 & 5/2 & 5 \\\\\n\\end{array}\\]\nIf we scale the term frequencies by the inverse document frequencies, we end with a feature representation where words that appear in fewer documents get higher scores.\nThe first column the remains the same, because its IDF is 1. The last column gets scaled up the most, so terrific now has the TFIDF score of 5, even though its count was just 1\n\\[\\begin{array}{cccccc} \\text{the} & \\text{a} & \\text{and} & \\text{might} & \\text{terrific}  \\\\\n5 & 5/2  & 0 & 0 & 5 \\\\\n2 & 15/4 & 5/3 & 0 & 0 \\\\\n3 & 5    & 0 & 0 & 0 \\\\\n2 & 10/4 & 5 & 5 & 0 \\\\\n1 & 0    & 5/3 & 5/2 & 0 \\\\\n\\end{array}\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#log-scaling",
    "href": "pages/lecturenotes/day4.html#log-scaling",
    "title": "Day 4",
    "section": "",
    "text": "Notice that in the example, the word terrific’s TFIDF score is \\(5\\times\\) higher than its count. If we had \\(N=1000\\) documents, and the word terrific only appeared once in one document, then IDF[terrific] would be \\(1000\\) even though its count is just \\(1\\)!\nThis is out of proportion. We want to scale up infrequent words, but not so much that they drown out all the other information.\nThe solution is to use logarithms. Instead of using the raw frequencies for TF and IDF, we log scale them. Here are the new definitions.\nThe raw counts are the numbers in the bag-of-words matrix.\n\\[count[d,t] = \\text{number of times term $t$ appears in document $d$}\\]\nThe term frequency is now 1 plus the log of the count. \\[TF[d,t] = 1 + \\log_{10}(count[d,t])\\] The document frequency is the number of documents a term occurs in, just as before. \\[DF[t] = \\text{number of documents term $t$ occurs in}\\] The inverse document frequency is now the log of the old document frequency (\\(N\\) over \\(DF[t]\\)]). \\[IDF[t] = \\log_{10}\\left(\\frac{N}{DF[t]}\\right)\\]"
  },
  {
    "objectID": "pages/lecturenotes/day4.html#for-next-time",
    "href": "pages/lecturenotes/day4.html#for-next-time",
    "title": "Day 4",
    "section": "",
    "text": "Next time we’ll see a worked-out example of TFIDF with log-scaling. We’ll also start using Python to calculate the model outputs."
  },
  {
    "objectID": "pages/lecturenotes/index.html",
    "href": "pages/lecturenotes/index.html",
    "title": "Lecture Notes",
    "section": "",
    "text": "Lecture Notes\n\nTuesday 09-03: Intro, Text as Strings, Classification, Sentiment Analysis, Features, Weights\n\n\nTuesday 09-05: Matrix Representations, Weights and Bias, Sigmoid, Model Predictions for Binary Classification"
  }
]