# Day 1

## What is a neural network, conceptually?

A neural network is a kind of function. It can take inputs in array format, and return outputs in a (possibly different) array format.

Neural networks can take many kinds of inputs, like text, images, or sound files. In each case, the input has to be encoded as an array of numbers.

```{mermaid}
flowchart LR
	A(Text) --> Z{Neural network}
	B(Image) --> Z
	C(Sound file) --> Z
	Z --> D("Label (Classification)")
	Z --> E("Text, Image, Sound (Generation)")
	
```
In the case of classification, the output is a label. In the case of generation, the output could be of many types, including more text, images, or sound.

## What is text?

On a computer, all text is an encoded sequence of characters. A *character* is a symbol like a letter or number, punctuation, spaces/tabs/newlines, or special characters like "?"" or emojis.

Unicode is the global standard for text encoding. It assigns every character to a code point. For example, capital `A` is `U+0041`, lowercase `a` is `U+0061`, and the number `1` is `U+0031`. Unicode covers all symbols in all scripts, plus many other characters. Some other examples are below.

| Symbol | Code point |
| ---- | ---- |
| ⽔ | U+2F54 |
| (Space) | U+0020 |
| अ | U+0905 |
| ? | U+003F |
You can search for any symbol you like [here](https://www.compart.com/en/unicode/) and find its Unicode code point.

Code points are turned into machine-readable representations by particular encoding schemes, which differ in how much memory they use. The most popular is [UTF-8](https://en.wikipedia.org/wiki/UTF-8) (read more about this on the Wiki link).

## Text classification

Let's take text classification as an example. In class we talked about sentiment analysis, spam detection, and topic modeling, but there are many other subtypes too.
```{mermaid}
flowchart TD
	A(Text Classification) --- B(Sentiment analysis)
	A --- C(Spam detection)
	A --- D(Topic modeling)
	A --- E(...)
```


In text classification, the input is a string (a sequence of characters). Let's take sentiment analysis as an example.

```{mermaid}
flowchart LR
	A("Text string <br> `This movie was great`") --> Z{Neural network}
	Z --> D("Positive ✓")
	Z --> E("Negative ✗")
```
In sentiment analysis, a model takes in a string and outputs a label, either `positive` or `negative`.

In practice, the model is rarely 100% sure about the label, so it outputs probabilities. For example, if `'This movie was great.'` is the input, the model might output `0.9` for `positive` and `0.1` for `negative`.

Next class, we'll see a worked out example with a simple toy model.

### Word embeddings

If we're doing text classification in English, then the *vocabulary* would consist of all English words (in the dictionary, let's say). 

Each word in the vocabulary is just a sequence of characters, as far as the computer is concerned. It doesn't know that `dog` is more similar in meaning to `canine` than it is to `philosophy`---all it sees are the characters.

We need a *feature representation* of words that captures similarities and differences in meaning. So, we encode every word as a *vector*, a list of numbers. These numbers can be thought of as coordinates for points in a high-dimensional space.

No matter how many dimensions a vector space has, we can always calculate the distance between points in the space.

### A visualization

The job of *vector space embedding* is to assign coordinates to every word in the vocabulary, and do this so that similar words are relatively close together and dissimilar words are relatively far apart.

To see an example, [here](https://projector.tensorflow.org/) are some 200 dimensional word embeddings that have been projected down into 3 dimensions. (Think about shadow puppets: 3D objects project as 2D shadows on a flat surface. This is kind of like looking at 3D shadows of 200D points.) 

You can zoom and pan around and look at which words are closer together.

## For next time

Next class, we will see a toy example of text classification with low-dimensional feature representations of strings. From there, we will look at how higher-dimensional feature representations help models understand more about the meaning of the text.
